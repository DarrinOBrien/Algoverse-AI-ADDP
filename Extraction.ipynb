{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Possible Installs - Runpod only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up Device + Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device) # https://github.com/openai/CLIP\n",
    "clip_model = clip_model.float() # For fp-32 precision\n",
    "mnist = load_dataset(\"ylecun/mnist\") # https://huggingface.co/datasets/ylecun/mnist\n",
    "split = mnist[\"train\"].train_test_split(test_size=0.2, seed=66)\n",
    "\n",
    "train_dataset = split[\"train\"] # 48,000 examples (direct training data from training set)\n",
    "val_dataset = split[\"test\"] # 12,000 examples (validation set split from training set)\n",
    "test_dataset = mnist[\"test\"] # 10,000 examples (direct test set)\n",
    "\n",
    "train_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "val_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapping Models & Prepping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "  def __init__(self, clip_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWithHooks(nn.Module):\n",
    "  def __init__(self, clip_model, classifier_head):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.cls_tokens = []\n",
    "    self.classifier = classifier_head\n",
    "\n",
    "  def forward(self, images):\n",
    "    self.cls_tokens = []\n",
    "\n",
    "    # B is batch\n",
    "    x = self.clip.visual.conv1(images)  # Convert image into patch embeddings. Divided into 32*32 patches. Shape is [B, 768, 7, 7]. Each 32*32 batch becomes a 768 dimensional vector. For 224*224 input, get 7*7=49 patches. Now have 49 such vectors per image.\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # -> [B, 768, 49] -> [B, 49, 768]; Each image is a sequence of 49 token vectors each of size 768, ready for the transformer.\n",
    "    x = x.permute(0,2,1)\n",
    "\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype) # Adds positional information so transformer knows order and position. [B, 50, 768] + [1, 50, 768]\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize to stablize it\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, 64, 768]\n",
    "\n",
    "    # Run resblocks manually, so hooks definitely trigger\n",
    "    for i, resblock in enumerate(self.clip.visual.transformer.resblocks):\n",
    "        x = resblock(x)\n",
    "        self.cls_tokens.append(x[0, :, :].detach())\n",
    "\n",
    "    x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None: # Linear Projection from 768 CLS token to 512 dimension vector for compatability\n",
    "      final_embed = x @ self.clip.visual.proj\n",
    "      final_embed = final_embed.detach()\n",
    "    else:\n",
    "      final_embed = x\n",
    "      final_embed = final_embed.detach()\n",
    "    \n",
    "    logits = self.classifier(final_embed)\n",
    "\n",
    "    return {\n",
    "      \"first_cls\": self.cls_tokens[0],\n",
    "      \"second_cls\": self.cls_tokens[1],\n",
    "      \"third_cls\": self.cls_tokens[2],\n",
    "      \"fourth_cls\": self.cls_tokens[3],\n",
    "      \"fifth_cls\": self.cls_tokens[4],\n",
    "      \"sixth_cls\": self.cls_tokens[5],\n",
    "      \"seventh_cls\": self.cls_tokens[6],\n",
    "      \"eighth_cls\": self.cls_tokens[7],\n",
    "      \"ninth_cls\": self.cls_tokens[8],\n",
    "      \"tenth_cls\": self.cls_tokens[9],\n",
    "      \"eleventh_cls\": self.cls_tokens[10],\n",
    "      \"twelfth_cls\": self.cls_tokens[11],\n",
    "      \"final_embed\": final_embed,\n",
    "      \"logits\": logits,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "base = CLIPWithHooks(copy.deepcopy(refer), nn.Linear(refer.visual.output_dim, 2)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = CLIPWithHooks(copy.deepcopy(refer), classifier_head=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for item in batch:\n",
    "    img = item[\"image\"].convert(\"RGB\")  # Already a PIL Image\n",
    "    img = preprocess(img)\n",
    "    images.append(img)\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  return {\n",
    "      \"pixel_values\": images.to(device),\n",
    "      \"labels\": labels.to(device)\n",
    "  }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collecting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = split[\"train\"].filter(lambda example: example[\"label\"] == 0)\n",
    "val_0 = split[\"test\"].filter(lambda example: example[\"label\"] == 0)\n",
    "test_0 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 0)\n",
    "\n",
    "train_0.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "val_0.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "test_0.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "train_0 = DataLoader(train_0, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "val_0 = DataLoader(val_0, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "test_0 = DataLoader(test_0, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least Squares Regression\n",
    "Z0_first_lsr = []\n",
    "Z0_second_lsr = []\n",
    "Z0_third_lsr = []\n",
    "Z0_fourth_lsr = []\n",
    "Z0_fifth_lsr = []\n",
    "\n",
    "Z1_twelfth_lsr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(tqdm(train_0, desc=\"Extracting Train Set Vectors\")):\n",
    "    images = batch[\"pixel_values\"]\n",
    "\n",
    "    out_base = base(images)\n",
    "    out_fine_tuned = fine_tuned(images)\n",
    "\n",
    "    Z0_first_lsr.append(out_base[\"first_cls\"].float())\n",
    "    Z0_second_lsr.append(out_base[\"second_cls\"].float())\n",
    "    Z0_third_lsr.append(out_base[\"third_cls\"].float())\n",
    "    Z0_fourth_lsr.append(out_base[\"fourth_cls\"].float())\n",
    "    Z0_fifth_lsr.append(out_base[\"fifth_cls\"].float())\n",
    "\n",
    "    Z1_twelfth_lsr.append(out_fine_tuned[\"twelfth_cls\"].float())\n",
    "\n",
    "Z0_first_lsr = torch.cat(Z0_first_lsr)\n",
    "Z0_second_lsr = torch.cat(Z0_second_lsr)\n",
    "Z0_third_lsr = torch.cat(Z0_third_lsr)\n",
    "Z0_fourth_lsr = torch.cat(Z0_fourth_lsr)\n",
    "Z0_fifth_lsr = torch.cat(Z0_fifth_lsr)\n",
    "\n",
    "Z1_twelfth_lsr = torch.cat(Z1_twelfth_lsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculating Linear Transformation Matrix and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal / Entire Dataset\n",
    "Z0_first_lsr_0 = Z0_first_lsr.cpu().numpy()\n",
    "Z0_second_lsr_0 = Z0_second_lsr.cpu().numpy()\n",
    "Z0_third_lsr_0 = Z0_third_lsr.cpu().numpy()\n",
    "Z0_fourth_lsr_0 = Z0_fourth_lsr.cpu().numpy()\n",
    "Z0_fifth_lsr_0 = Z0_fifth_lsr.cpu().numpy()\n",
    "\n",
    "Z1_twelfth_lsr_0 = Z1_twelfth_lsr.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constant 1 column for bias term\n",
    "def addBiasColumn(Z0):\n",
    "    ones = np.ones((Z0.shape[0], 1)) # (1,1)\n",
    "    return np.hstack([Z0, ones])\n",
    "\n",
    "def leastSquares(Z0, Z1):\n",
    "    W_full, residuals, rank, s = np.linalg.lstsq(Z0, Z1, rcond=None)\n",
    "    return W_full  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_avg(Z0, Z1):\n",
    "    Z0 = addBiasColumn(Z0)\n",
    "    full = leastSquares(Z0, Z1)\n",
    "    W = full[:-1]\n",
    "    b = full[-1]\n",
    "    return W, b\n",
    "\n",
    "W_first_0, b_first_0 = retrieve_avg(Z0_first_lsr_0, Z1_twelfth_lsr_0)\n",
    "W_second_0, b_second_0 = retrieve_avg(Z0_second_lsr_0, Z1_twelfth_lsr_0)\n",
    "W_third_0, b_third_0 = retrieve_avg(Z0_third_lsr_0, Z1_twelfth_lsr_0)\n",
    "W_fourth_0, b_fourth_0 = retrieve_avg(Z0_fourth_lsr_0, Z1_twelfth_lsr_0)\n",
    "W_fifth_0, b_fifth_0 = retrieve_avg(Z0_fifth_lsr_0, Z1_twelfth_lsr_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extra Split Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_1 = split[\"train\"].filter(lambda example: example[\"label\"] == 1)\n",
    "# val_1 = split[\"test\"].filter(lambda example: example[\"label\"] == 1)\n",
    "# test_1 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 1)\n",
    "\n",
    "# train_2 = split[\"train\"].filter(lambda example: example[\"label\"] == 2)\n",
    "# val_2 = split[\"test\"].filter(lambda example: example[\"label\"] == 2)\n",
    "# test_2 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 2)\n",
    "\n",
    "# train_3 = split[\"train\"].filter(lambda example: example[\"label\"] == 3)\n",
    "# val_3 = split[\"test\"].filter(lambda example: example[\"label\"] == 3)\n",
    "# test_3 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 3)\n",
    "\n",
    "# train_4 = split[\"train\"].filter(lambda example: example[\"label\"] == 4)\n",
    "# val_4 = split[\"test\"].filter(lambda example: example[\"label\"] == 4)\n",
    "# test_4 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 4)\n",
    "\n",
    "# train_5 = split[\"train\"].filter(lambda example: example[\"label\"] == 5)\n",
    "# val_5 = split[\"test\"].filter(lambda example: example[\"label\"] == 5)\n",
    "# test_5 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 5)\n",
    "\n",
    "# train_6 = split[\"train\"].filter(lambda example: example[\"label\"] == 6)\n",
    "# val_6 = split[\"test\"].filter(lambda example: example[\"label\"] == 6)\n",
    "# test_6 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 6)\n",
    "\n",
    "# train_7 = split[\"train\"].filter(lambda example: example[\"label\"] == 7)\n",
    "# val_7 = split[\"test\"].filter(lambda example: example[\"label\"] == 7)\n",
    "# test_7 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 7)\n",
    "\n",
    "# train_8 = split[\"train\"].filter(lambda example: example[\"label\"] == 8)\n",
    "# val_8 = split[\"test\"].filter(lambda example: example[\"label\"] == 8)\n",
    "# test_8 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 8)\n",
    "\n",
    "# train_9 = split[\"train\"].filter(lambda example: example[\"label\"] == 9)\n",
    "# val_9 = split[\"test\"].filter(lambda example: example[\"label\"] == 9)\n",
    "# test_9 = mnist[\"test\"].filter(lambda example: example[\"label\"] == 9)\n",
    "\n",
    "# train_1.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_1.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_1.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_2.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_2.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_2.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_3.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_3.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_3.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_4.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_4.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_4.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_5.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_5.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_5.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_6.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_6.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_6.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_7.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_7.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_7.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_8.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_8.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_8.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_9.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# val_9.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "# test_9.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "# train_1 = DataLoader(train_1, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_1 = DataLoader(val_1, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_1 = DataLoader(test_1, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_2 = DataLoader(train_2, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_2 = DataLoader(val_2, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_2 = DataLoader(test_2, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_3 = DataLoader(train_3, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_3 = DataLoader(val_3, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_3 = DataLoader(test_3, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_4 = DataLoader(train_4, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_4 = DataLoader(val_4, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_4 = DataLoader(test_4, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_5 = DataLoader(train_5, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_5 = DataLoader(val_5, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_5 = DataLoader(test_5, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_6 = DataLoader(train_6, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_6 = DataLoader(val_6, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_6 = DataLoader(test_6, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_7 = DataLoader(train_7, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_7 = DataLoader(val_7, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_7 = DataLoader(test_7, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_8 = DataLoader(train_8, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_8 = DataLoader(val_8, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_8 = DataLoader(test_8, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "\n",
    "# train_9 = DataLoader(train_9, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "# val_9 = DataLoader(val_9, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "# test_9 = DataLoader(test_9, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manipulating Base CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes) # 512 -> 10\n",
    "    \n",
    "    def forward(self, images):\n",
    "        image_features = self.clip.encode_image(images)\n",
    "        logits = self.classifier(image_features)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedCLIP(nn.Module):\n",
    "    def __init__(self, clip, W=None, b=None, transform_stage=None, classifier=None):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.b = torch.from_numpy(b.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.transform_stage = transform_stage if transform_stage is not None else -1\n",
    "        self.classifier = classifier if classifier is not None else nn.Linear(self.clip.visual.output_dim, 2)\n",
    "    \n",
    "    def forward(self, image): \n",
    "        image = image.to(device)\n",
    "        x = self.clip.visual.conv1(image)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "        x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "        x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "        x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, 64, 768]\n",
    "\n",
    "        if self.transform_stage == -1:\n",
    "            x = self.clip.visual.transformer(x)\n",
    "        else:\n",
    "            for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "                x = block(x)\n",
    "                if i+1 == self.transform_stage:\n",
    "                    cls = x[0, :, :]\n",
    "                    cls = cls.to(torch.float32)\n",
    "                    manipulated = cls @ self.W + self.b\n",
    "                    break\n",
    "            manipulated = manipulated.unsqueeze(0) # Shape (1, B, D)\n",
    "            x = torch.cat([manipulated, x[1:, :, :]], dim=0) # Adds manipulated cls token all together, not seperately\n",
    "        twelfth_cls = x[0, :, :].squeeze()\n",
    "        \n",
    "        x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "        x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.clip.visual.proj is not None:\n",
    "            final_embed = x @ self.clip.visual.proj\n",
    "            final_embed = final_embed\n",
    "        else:\n",
    "            final_embed = x\n",
    "            final_embed = final_embed\n",
    "        \n",
    "        logits = self.classifier(final_embed)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"final_embed\": final_embed,\n",
    "            \"manipulated_cls\": manipulated.squeeze(0) if self.transform_stage != -1 else twelfth_cls\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = AugmentedCLIP(copy.deepcopy(refer), classifier=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval()\n",
    "\n",
    "base = AugmentedCLIP(copy.deepcopy(refer), classifier=f_t.classifier)\n",
    "base = base.eval()\n",
    "\n",
    "aug_0_1 = AugmentedCLIP(copy.deepcopy(refer), W=W_first_0, b=b_first_0, transform_stage=1, classifier=f_t.classifier)\n",
    "aug_0_1.eval()\n",
    "\n",
    "aug_0_2 = AugmentedCLIP(copy.deepcopy(refer), W=W_second_0, b=b_second_0, transform_stage=2, classifier=f_t.classifier)\n",
    "aug_0_2.eval()\n",
    "\n",
    "aug_0_3 = AugmentedCLIP(copy.deepcopy(refer), W=W_third_0, b=b_third_0, transform_stage=3, classifier=f_t.classifier)\n",
    "aug_0_3.eval()\n",
    "\n",
    "aug_0_4 = AugmentedCLIP(copy.deepcopy(refer), W=W_fourth_0, b=b_fourth_0, transform_stage=4, classifier=f_t.classifier)\n",
    "aug_0_4.eval()\n",
    "\n",
    "aug_0_5 = AugmentedCLIP(copy.deepcopy(refer), W=W_fifth_0, b=b_fifth_0, transform_stage=5, classifier=f_t.classifier)\n",
    "aug_0_5.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "correct_3 = 0\n",
    "correct_4 = 0\n",
    "correct_5 = 0\n",
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "sim_final_1 = []\n",
    "sim_cls_1 = []\n",
    "\n",
    "sim_final_2 = []\n",
    "sim_cls_2 = []\n",
    "\n",
    "sim_final_3 = []\n",
    "sim_cls_3 = []\n",
    "\n",
    "sim_final_4 = []\n",
    "sim_cls_4 = []\n",
    "\n",
    "sim_final_5 = []\n",
    "sim_cls_5 = []\n",
    "\n",
    "def calcPred(model, images):\n",
    "    logits = model(images)[\"logits\"]\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return pred\n",
    "\n",
    "def cosineSimilarity(aug, fine, images):\n",
    "    out_aug = aug(images)\n",
    "    out_aug_embed = out_aug[\"final_embed\"]\n",
    "    out_aug_cls = out_aug[\"manipulated_cls\"]\n",
    "\n",
    "    out_fine = fine(images)\n",
    "    out_fine_embed = out_fine[\"final_embed\"]\n",
    "    out_fine_cls = out_fine[\"manipulated_cls\"]\n",
    "\n",
    "    # Prevent NaNs\n",
    "    eps = 1e-8\n",
    "    out_aug_embed = F.normalize(out_aug_embed, dim=1, eps=eps)\n",
    "    out_fine_embed = F.normalize(out_fine_embed, dim=1, eps=eps)\n",
    "    out_aug_cls = F.normalize(out_aug_cls, dim=1, eps=eps)\n",
    "    out_fine_cls = F.normalize(out_fine_cls, dim=1, eps=eps)\n",
    "\n",
    "    cos_sim_final = (out_aug_embed * out_fine_embed).sum(dim=1).mean().item()\n",
    "    cos_sim_cls = (out_aug_cls * out_fine_cls).sum(dim=1).mean().item()\n",
    "    return cos_sim_final, cos_sim_cls\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_0, desc=\"Evaluating 0's\"):\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        correct_1 += (calcPred(aug_0_1, images) == labels).sum().item()\n",
    "        correct_2 += (calcPred(aug_0_2, images) == labels).sum().item()\n",
    "        correct_3 += (calcPred(aug_0_3, images) == labels).sum().item()\n",
    "        correct_4 += (calcPred(aug_0_4, images) == labels).sum().item()\n",
    "        correct_5 += (calcPred(aug_0_5, images) == labels).sum().item()\n",
    "\n",
    "        correct_base += (calcPred(base, images) == labels).sum().item()\n",
    "        correct_fine_tuned += (calcPred(fine_tuned, images) == labels).sum().item()\n",
    "\n",
    "        cs_final, cs_cls = cosineSimilarity(aug_0_1, fine_tuned, images)\n",
    "        sim_final_1.append(cs_final)\n",
    "        sim_cls_1.append(cs_cls)\n",
    "\n",
    "        cs_final, cs_cls = cosineSimilarity(aug_0_2, fine_tuned, images)\n",
    "        sim_final_2.append(cs_final)\n",
    "        sim_cls_2.append(cs_cls)\n",
    "\n",
    "        cs_final, cs_cls = cosineSimilarity(aug_0_3, fine_tuned, images)\n",
    "        sim_final_3.append(cs_final)\n",
    "        sim_cls_3.append(cs_cls)\n",
    "\n",
    "        cs_final, cs_cls = cosineSimilarity(aug_0_4, fine_tuned, images)\n",
    "        sim_final_4.append(cs_final)\n",
    "        sim_cls_4.append(cs_cls)\n",
    "\n",
    "        cs_final, cs_cls = cosineSimilarity(aug_0_5, fine_tuned, images)\n",
    "        sim_final_5.append(cs_final)\n",
    "        sim_cls_5.append(cs_cls)\n",
    "\n",
    "acc_1 = correct_1 / total_samples\n",
    "acc_2 = correct_2 / total_samples\n",
    "acc_3 = correct_3 / total_samples\n",
    "acc_4 = correct_4 / total_samples\n",
    "acc_5 = correct_5 / total_samples\n",
    "\n",
    "acc_base = correct_base / total_samples\n",
    "acc_fine_tuned = correct_fine_tuned / total_samples\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\\n\")\n",
    "for i, acc in enumerate([\n",
    "    acc_1, acc_2, acc_3, acc_4, acc_5\n",
    "]):\n",
    "    print(f\"Augmented {i+1} - Last Layer CLIP Accuracy: {acc:.4f}\")\n",
    "print(f\"Base CLIP Accuracy: {acc_base:.4f}\")\n",
    "print(f\"Fine-Tuned CLIP Accuracy: {acc_fine_tuned:.4f}\")\n",
    "\n",
    "for i, fin in enumerate([\n",
    "    sim_final_1, sim_final_2, sim_final_3, sim_final_4, sim_final_5,\n",
    "]):\n",
    "    print(f\"Avg cos sim final embed for aug_0_{i+1}: {np.mean(fin):.4f}\")\n",
    "\n",
    "for i, cls in enumerate([\n",
    "    sim_cls_1, sim_cls_2, sim_cls_3, sim_cls_4, sim_cls_5,\n",
    "]):\n",
    "    print(f\"Avg cos sim CLS token for aug_0_{i+1}: {np.mean(cls):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
