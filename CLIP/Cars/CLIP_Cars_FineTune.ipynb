{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "Author: Darrin O'Brien, email: darrinobrien5@gmail.com\n",
    "\n",
    "**Note: Not Needed. Reference if have time**\n",
    "1. Fine-Tunes CLIP ViT-32 on the Stanford Cars Dataset (https://docs.pytorch.org/vision/main/generated/torchvision.datasets.StanfordCars.html). Original Paper: https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html \n",
    "2. Evaluates the performance of the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Installs for Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install pandas scipy\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Runpod Only Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import StanfordCars\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up Device and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device) # https://github.com/openai/CLIP\n",
    "clip_model = clip_model.float() # For fp-32 precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install kagglehub\\nimport os\\nimport json\\nimport kagglehub\\nfrom scipy.io import loadmat\\nfrom sklearn.model_selection import train_test_split\\nimport pandas as pd\\n\\npath = \"../../kaggle.json\" # Locally\\n# path = \"kaggle.json\"\\nwith open(path, \"r\") as f:\\n    kaggle_creds = json.load(f)\\n\\n# Set environment variables\\nos.environ[\"KAGGLE_USERNAME\"] = kaggle_creds[\"username\"]\\nos.environ[\"KAGGLE_KEY\"] = kaggle_creds[\"key\"]\\n\\npath = kagglehub.dataset_download(\"eduardo4jesus/stanford-cars-dataset\")\\n\\nprint(\"Path to dataset files:\", path) # Then move this into the directory\\n\\nload_train, load_test = loadmat(f\"stanford-cars-dataset/car_devkit/devkit/cars_train_annos.mat\"), loadmat(f\"stanford-cars-dataset/car_devkit/devkit/cars_test_annos.mat\")\\nannot_train, annot_test = load_train[\"annotations\"][0], load_test[\"annotations\"][0]\\n\\nall_train = pd.DataFrame([\\n    {\\n        \\'fname\\': str(i[\\'fname\\'][0]), # File Name\\n        \\'class\\': int(i[\"class\"][0][0]) # ID of Class\\n    }\\n    for i in annot_train\\n])\\n\\ntest = pd.DataFrame([\\n    {\\n        \\'fname\\': str(i[\\'fname\\'][0]), # File Name\\n        \\'class\\': int(i[\"class\"][0][0]) # ID of Class\\n    }\\n    for i in annot_test\\n])\\n\\n# train, val = train_test_split(all_train, test_size=0.2)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only needed if going off of Kaggle. However, there is no test labels so you can't verify learning unless you minimize the training data. \n",
    "'''\n",
    "!pip install kagglehub\n",
    "import os\n",
    "import json\n",
    "import kagglehub\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "path = \"../../kaggle.json\" # Locally\n",
    "# path = \"kaggle.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"KAGGLE_USERNAME\"] = kaggle_creds[\"username\"]\n",
    "os.environ[\"KAGGLE_KEY\"] = kaggle_creds[\"key\"]\n",
    "\n",
    "path = kagglehub.dataset_download(\"eduardo4jesus/stanford-cars-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path) # Then move this into the directory\n",
    "\n",
    "load_train, load_test = loadmat(f\"stanford-cars-dataset/car_devkit/devkit/cars_train_annos.mat\"), loadmat(f\"stanford-cars-dataset/car_devkit/devkit/cars_test_annos.mat\")\n",
    "annot_train, annot_test = load_train[\"annotations\"][0], load_test[\"annotations\"][0]\n",
    "\n",
    "all_train = pd.DataFrame([\n",
    "    {\n",
    "        'fname': str(i['fname'][0]), # File Name\n",
    "        'class': int(i[\"class\"][0][0]) # ID of Class\n",
    "    }\n",
    "    for i in annot_train\n",
    "])\n",
    "\n",
    "test = pd.DataFrame([\n",
    "    {\n",
    "        'fname': str(i['fname'][0]), # File Name\n",
    "        'class': int(i[\"class\"][0][0]) # ID of Class\n",
    "    }\n",
    "    for i in annot_test\n",
    "])\n",
    "\n",
    "# train, val = train_test_split(all_train, test_size=0.2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = StanfordCars(root=\"./data\", split=\"train\", download=True)\n",
    "test = StanfordCars(root=\"./data\", split=\"test\", download=True)\n",
    "\n",
    "labels = [full_train[i][1] for i in range(len(full_train))]\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    [i for i in range(len(full_train))],\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=66,\n",
    ")\n",
    "\n",
    "train = Subset(full_train, train_indices)\n",
    "val = Subset(full_train, val_indices)\n",
    "\n",
    "def clip_collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img, label in batch:\n",
    "        img = preprocess(img)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": images.to(device),\n",
    "        \"labels\": labels.to(device)\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=True, num_workers=4, collate_fn=clip_collate_fn)\n",
    "val_loader = DataLoader(val, batch_size=64, shuffle=False, num_workers=4, collate_fn=clip_collate_fn)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False, num_workers=4, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tune Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "  def __init__(self, clip_model, num_classes=196):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits\n",
    "\n",
    "model = CLIPClassifier(clip_model=clip_model).to(device)\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "  model = model.float()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "EPOCHS = 15 # For fp-32\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning CLIP on Stanford Cars Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f\"Epoch {epoch+1}/{EPOCHS} - Best Val Loss: {best_val_loss:.4f} (Epoch {best_epoch})\")\n",
    "\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  train_steps = 0\n",
    "\n",
    "  for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    images = batch[\"pixel_values\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    logits = model(images)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_train_loss += loss.item()\n",
    "    train_steps += 1\n",
    "\n",
    "  avg_train_loss = total_train_loss / train_steps\n",
    "\n",
    "  # Validation\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  total_val_loss = 0\n",
    "  val_steps = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "      images = batch[\"pixel_values\"]\n",
    "      labels = batch[\"labels\"]\n",
    "\n",
    "      logits = model(images)\n",
    "      loss = criterion(logits, labels)\n",
    "\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "      correct += (preds == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      val_steps += 1\n",
    "\n",
    "  avg_val_loss = total_val_loss / val_steps\n",
    "  val_acc = correct / total\n",
    "\n",
    "  print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "  if avg_val_loss < best_val_loss:\n",
    "    best_val_loss = avg_val_loss\n",
    "    best_epoch = epoch\n",
    "    torch.save(model.state_dict(), \"best_clip_cars.pt\")\n",
    "\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_CLIP, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "base_CLIP = base_CLIP.float() # fp-32\n",
    "model = CLIPClassifier(clip_model=base_CLIP).to(device)\n",
    "\n",
    "best_CLIP, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "best_CLIP = best_CLIP.float() # fp-32\n",
    "best_CLIP_Cars = CLIPClassifier(clip_model=best_CLIP).to(device)\n",
    "best_CLIP_Cars.load_state_dict(torch.load(\"best_clip_cars.pt\", map_location=device)) # map_location tells where to place the model's weights in memory\n",
    "\n",
    "model.eval()\n",
    "best_CLIP_Cars.eval()\n",
    "\n",
    "total_test_loss_base = 0\n",
    "total_base = 0\n",
    "total_test_loss_best = 0\n",
    "total_best = 0\n",
    "\n",
    "correct_base = 0\n",
    "correct_best = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    images = batch[\"pixel_values\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    # Base model\n",
    "    logits_base = model(images)\n",
    "    loss_base = criterion(logits_base, labels)\n",
    "    total_test_loss_base += loss_base.item()\n",
    "    total_base += 1\n",
    "\n",
    "    # Best model\n",
    "    logits_best = best_CLIP_Cars(images)\n",
    "    loss_best = criterion(logits_best, labels)\n",
    "    total_test_loss_best += loss_best.item()\n",
    "    total_best += 1\n",
    "\n",
    "    # Classification Accuracy\n",
    "    pred_base = logits_base.argmax(dim=1)\n",
    "    pred_best = logits_best.argmax(dim=1)\n",
    "\n",
    "    correct_base += (pred_base == labels).sum().item()\n",
    "    correct_best += (pred_best == labels).sum().item()\n",
    "\n",
    "avg_base_loss = total_test_loss_base / total_base\n",
    "avg_best_loss = total_test_loss_best / total_best\n",
    "\n",
    "accuracy_base = correct_base / total_samples\n",
    "accuracy_best = correct_best / total_samples\n",
    "print(f\"\\nAverage base loss: {avg_base_loss:.4f}, Base Accuracy: {accuracy_base:.4f}\")\n",
    "print(f\"Average best loss: {avg_best_loss:.4f}, Best Accuracy: {accuracy_best:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
