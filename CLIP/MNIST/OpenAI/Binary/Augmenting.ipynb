{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "Author: Darrin O'Brien, email darrinobrien5@gmail.com\n",
    "\n",
    "**Note: No need for testing anymore; Translation Vector is enough to replicate performance with only 1 image for binary classification**\n",
    "1. Preparation.\n",
    "2. Loads Base and Fine-Tuned on MNIST CLIP Models.\n",
    "3. Seperates Test Set into 10 different subsets, each of the number {1,2,3...9}. So one testset is only comprised of a single label, e.g. label 0. Option to minimize training dataset.\n",
    "3. Extracts Image Embedding Vectors of both models on test sets. From base layers {1,2,...5} to fine-tuned last layer's transformer layer 12. \n",
    "4. Applys learned weight W and bias b terms to augment base CLIP in 5 different ways.  \n",
    "5. Evaluates the performance of the augmented models in comparison to the base and fine-tuned models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extra Installs for Runpod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Setting up Device + Entire Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affine = True\n",
    "Transformation_Matrix = False\n",
    "Translation_Vector = False\n",
    "\n",
    "type = \"\"\n",
    "\n",
    "if Affine:\n",
    "    type = \"Affine_W_b\"\n",
    "elif Transformation_Matrix:\n",
    "    type = \"Transformation_Matrix_W\"\n",
    "elif Translation_Vector:\n",
    "    type = \"Translation_Vector_b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device) # https://github.com/openai/CLIP\n",
    "clip_model = clip_model.float().to(device) # For fp-32 precision\n",
    "mnist = load_dataset(\"ylecun/mnist\") # https://huggingface.co/datasets/ylecun/mnist\n",
    "split = mnist[\"train\"].train_test_split(test_size=0.2, seed=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapper Class for Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module): # for fine-tuned model\n",
    "  def __init__(self, clip_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns All CLS Tokens + Final Embedding + Logits \n",
    "class CLIPWithHooks(nn.Module):\n",
    "  def __init__(self, clip_model, classifier_head):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.cls_tokens = []\n",
    "    self.classifier = classifier_head\n",
    "\n",
    "  def forward(self, images):\n",
    "    self.cls_tokens = []\n",
    "\n",
    "    # B is batch\n",
    "    x = self.clip.visual.conv1(images)  # Convert image into patch embeddings. Divided into 32*32 patches. Shape is [B, 768, 7, 7]. Each 32*32 batch becomes a 768 dimensional vector. For 224*224 input, get 7*7=49 patches. Now have 49 such vectors per image.\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # -> [B, 768, 49] -> [B, 49, 768]; Each image is a sequence of 49 token vectors each of size 768, ready for the transformer.\n",
    "    x = x.permute(0,2,1)\n",
    "\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype) # Adds positional information so transformer knows order and position. [B, 50, 768] + [1, 50, 768]\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize to stablize it\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, 64, 768]\n",
    "\n",
    "    # Run resblocks manually, so hooks definitely trigger\n",
    "    for i, resblock in enumerate(self.clip.visual.transformer.resblocks):\n",
    "        x = resblock(x)\n",
    "        self.cls_tokens.append(x[0, :, :].detach())\n",
    "\n",
    "    x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None: # Linear Projection from 768 CLS token to 512 dimension vector for compatability\n",
    "      final_embed = x @ self.clip.visual.proj\n",
    "      final_embed = final_embed.detach()\n",
    "    else:\n",
    "      final_embed = x\n",
    "      final_embed = final_embed.detach()\n",
    "    \n",
    "    logits = self.classifier(final_embed)\n",
    "\n",
    "    return {\n",
    "      \"cls\": [i for i in self.cls_tokens],\n",
    "      \"logits\": logits,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float().to(device)\n",
    "\n",
    "base = CLIPWithHooks(copy.deepcopy(refer), nn.Linear(refer.visual.output_dim, 2)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval().to(device)\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = CLIPWithHooks(f_t.clip, classifier_head=f_t.classifier) # Load in fine-tuned model with fine-tuned visual encoder. Basically just the fine-tuned model's visual and text encoder.\n",
    "fine_tuned = fine_tuned.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for item in batch:\n",
    "    img = item[\"image\"].convert(\"RGB\")  # Already a PIL Image\n",
    "    img = preprocess(img)\n",
    "    images.append(img)\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  return {\n",
    "      \"pixel_values\": images.to(device),\n",
    "      \"labels\": labels.to(device)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting up Individual Label Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = {}\n",
    "val_loader = {}\n",
    "test_loader = {}\n",
    "\n",
    "size_nums = [float('inf'), 600, 300, 100, 50, 25, 10, 5, 1]\n",
    "size_indice = 0\n",
    "train_size = 0\n",
    "\n",
    "# 0-9 Labels\n",
    "for i in range(10):\n",
    "    train_dataset = split[\"train\"].filter(lambda example: example[\"label\"] == i)\n",
    "    val_dataset = split[\"test\"].filter(lambda example: example[\"label\"] == i)\n",
    "    test_dataset = mnist[\"test\"].filter(lambda example: example[\"label\"] == i)\n",
    "\n",
    "    num = min(size_nums[size_indice], len(train_dataset))\n",
    "    train_dataset = train_dataset.select(range(num))\n",
    "    train_size += num\n",
    "\n",
    "    train_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "    val_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "    test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "    train_loader[i] = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "    val_loader[i] = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "    test_loader[i] = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Least Squares\n",
    "def leastSquares(Z0, Z1):\n",
    "    W_full, residuals, rank, s = np.linalg.lstsq(Z0, Z1, rcond=None)\n",
    "    return W_full  \n",
    "\n",
    "def addBiasColumn(Z0):\n",
    "    ones = np.ones((Z0.shape[0], 1)) # (1,1)\n",
    "    return np.hstack([Z0, ones])\n",
    "\n",
    "def retrieve_avg(Z0, Z1):\n",
    "    if Affine:\n",
    "        Z0 = addBiasColumn(Z0)\n",
    "        full = leastSquares(Z0, Z1)\n",
    "        W = full[:-1]\n",
    "        b = full[-1]\n",
    "        return W, b\n",
    "    if Transformation_Matrix:\n",
    "        return leastSquares(Z0, Z1), None\n",
    "    if Translation_Vector:\n",
    "        Z0 = np.array(Z0)\n",
    "        Z1 = np.array(Z1)\n",
    "        diff = Z1 - Z0\n",
    "        return np.mean(diff, axis=0), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-9 labels,\n",
    "W = {}\n",
    "b = {}\n",
    "for i in range(10):\n",
    "    W[i] = {}\n",
    "    b[i] = {}\n",
    "\n",
    "# For Labels 0-9\n",
    "for i in range(10):\n",
    "    # Least Squares Regression\n",
    "    Z0 = {}\n",
    "    for j in range(0, 12, 2): # {0,2,4,6,8,10}\n",
    "        Z0[i] = []\n",
    "    Z0[11] = [] \n",
    "\n",
    "    Z1_twelfth_lsr = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader[i], desc=f\"Extracting Train Set Label = {i} Vectors\"):\n",
    "            images = batch[\"pixel_values\"]\n",
    "\n",
    "            out_base = base(images)\n",
    "            out_fine_tuned = fine_tuned(images)\n",
    "            \n",
    "            for j in range(0, 12, 2):\n",
    "                Z0[j].append(out_base[\"cls\"][j].float())\n",
    "            Z0[11].append(out_base[\"cls\"][11].float())\n",
    "\n",
    "            Z1_twelfth_lsr.append(out_fine_tuned[\"cls\"][11].float())\n",
    "\n",
    "    Z1_twelfth_lsr = torch.cat(Z1_twelfth_lsr)\n",
    "    Z1_twelfth_lsr = Z1_twelfth_lsr.cpu().numpy()\n",
    "\n",
    "    temp_W = {}\n",
    "    temp_b = {}\n",
    "    for key, value in Z0.items():\n",
    "        value = torch.cat(value)\n",
    "        value = value.cpu().numpy()\n",
    "        temp_W[key], temp_b[key] = retrieve_avg(value, Z1_twelfth_lsr)\n",
    "    \n",
    "    W[i] = temp_W\n",
    "    b[i] = temp_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrapper Classes for Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedCLIP(nn.Module):\n",
    "    def __init__(self, clip, W=None, b=None, transform_stage=None, classifier=None):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.b = torch.from_numpy(b.astype(np.float32)).to(device) if b is not None else None\n",
    "        self.transform_stage = transform_stage if transform_stage is not None else -1\n",
    "        self.classifier = classifier if classifier is not None else nn.Linear(self.clip.visual.output_dim, 10)\n",
    "    \n",
    "    def forward(self, image): \n",
    "        image = image.to(device)\n",
    "        x = self.clip.visual.conv1(image)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "        x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "        x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "        x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, 64, 768]\n",
    "\n",
    "        if self.transform_stage == -1:\n",
    "            x = self.clip.visual.transformer(x)\n",
    "        else:\n",
    "            for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "                x = block(x)\n",
    "                if i == self.transform_stage:\n",
    "                    cls = x[0, :, :]\n",
    "                    cls = cls.to(torch.float32)\n",
    "                    if self.W is None:\n",
    "                        self.W = torch.eye(cls.shape[-1], device=cls.device, dtype=cls.dtype)\n",
    "                    if self.b is None:\n",
    "                        self.b = torch.zeros(cls.shape[-1], device=cls.device, dtype=cls.dtype)\n",
    "                    manipulated = cls @ self.W + self.b\n",
    "                    break\n",
    "            manipulated = manipulated.unsqueeze(0) # Shape (1, B, D)\n",
    "            x = torch.cat([manipulated, x[1:, :, :]], dim=0) # Adds manipulated cls token all together, not seperately\n",
    "        \n",
    "        twelfth_cls = x[0, :, :].squeeze()\n",
    "        \n",
    "        x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "        x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.clip.visual.proj is not None:\n",
    "            final_embed = x @ self.clip.visual.proj\n",
    "        else:\n",
    "            final_embed = x\n",
    "        \n",
    "        logits = self.classifier(final_embed)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"manipulated_cls\": manipulated.squeeze(0) if self.transform_stage != -1 else twelfth_cls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = AugmentedCLIP(f_t.clip, classifier=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval().to(device)\n",
    "\n",
    "base_classifier = nn.Linear(refer.visual.output_dim, 2) # Binary Class. 50% chance. \n",
    "base = AugmentedCLIP(copy.deepcopy(refer), classifier=base_classifier)\n",
    "base = base.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = {{} for i in range(10)}\n",
    "\n",
    "# 70 augmented models {0,2,4,6,8,10,11}\n",
    "for i in range(10):\n",
    "    temp = {}\n",
    "    for j in range(0, 12, 2):\n",
    "        model = AugmentedCLIP(copy.deepcopy(refer), W=W[i], b=b[i], transform_stage=i,classifier=f_t.classifier)\n",
    "        model = model.eval().to(device)\n",
    "        temp[i] = model\n",
    "    m11 = AugmentedCLIP(copy.deepcopy(refer), W=W[11], b=b[11], transform_stage=11, classifer=f_t.classifier)\n",
    "    m11 = m11.eval().to(device)\n",
    "    temp[11] = m11\n",
    "    aug[i] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating Augmented Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPred(model, images):\n",
    "    logits = model(images)[\"logits\"]\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return pred\n",
    "\n",
    "def cosineSimilarity(aug, fine, images):\n",
    "    out_aug = aug(images)\n",
    "    out_aug_cls = out_aug[\"manipulated_cls\"]\n",
    "\n",
    "    out_fine = fine(images)\n",
    "    out_fine_cls = out_fine[\"manipulated_cls\"] # Not Actually manipulated. Just changed. \n",
    "\n",
    "    # Prevent NaNs\n",
    "    eps = 1e-8\n",
    "    out_aug_cls = F.normalize(out_aug_cls, dim=1, eps=eps)\n",
    "    out_fine_cls = F.normalize(out_fine_cls, dim=1, eps=eps)\n",
    "\n",
    "    cos_sim_cls = (out_aug_cls * out_fine_cls).sum(dim=1).mean().item()\n",
    "    return cos_sim_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_fine = {i for i in range(10)}\n",
    "correct_base = {i for i in range(10)}\n",
    "correct = {i for i in range(10)}\n",
    "sim_cls = {i for i in range(10)}\n",
    "\n",
    "for i in range(10):\n",
    "    temp_correct_base = 0\n",
    "    temp_correct_fine_tuned = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    temp_c = {}\n",
    "    temp_cls = {}\n",
    "    for j in range(0, 12, 2):\n",
    "        temp_c[i] = 0\n",
    "        temp_cls[i] = []\n",
    "    temp_c[11] = 0\n",
    "    temp_cls[11] = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader[i], desc=f\"Evaluating Augmented Model Label {i}\"):\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            correct_base[i] = (calcPred())\n",
    "            \n",
    "            for j in range(0,12,2):\n",
    "                temp_c[j] += (calcPred(aug[i][j], images) == labels).sum().item()\n",
    "                temp_cls[j].append(cosineSimilarity(aug[i][j], fine_tuned, images))\n",
    "            temp_c[11] += (calcPred(aug[i][11], images) == labels).sum().item()\n",
    "            temp_cls[11].append(cosineSimilarity(aug[i][11], fine_tuned, images))\n",
    "\n",
    "    for j in range(0,12,2):\n",
    "        temp_c[j] = temp_c[j] / total_samples\n",
    "        temp_cls[j] = np.mea(temp_cls[j])\n",
    "    \n",
    "    correct[i] = temp_c\n",
    "    sim_cls[i] = temp_cls \n",
    "    correct_fine[i] = temp_correct_fine_tuned / correct_base \n",
    "    correct_base[i] = temp_correct_base / correct_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Augmented CLIP on MNIST Label {i} Results\")\n",
    "    for j in range(0,12,2):\n",
    "        print(f\"\\tAugmented {j} - Last (12) Layer Accuracy: {correct[i][j]}\")\n",
    "        print(f\"\\tAverage Cosine Similarity of CLS Token of Augmented 12 Layer: {sim_cls[i][j]:.4f}\")\n",
    "    print(f\"\\tAugmented 12 - Last (12) Layer Accuracy: {correct[i][11]}\")\n",
    "    print(f\"\\tAverage Cosine Similarity of CLS Token of Augmented 12 Layer: {sim_cls[i][j]:.4f}\\n\")\n",
    "    print(f\"\\tBase Accuracy: {correct_base[i]:.4f}\")\n",
    "    print(f\"\\tFine-Tuned Accuracy: {correct_fine[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look towards saving them either in seperate files or in random dictionaries\n",
    "folder = f\"./Binary_{type}\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "for i in range(10):\n",
    "\n",
    "    sub = f\"{folder}/Label{i}\"\n",
    "\n",
    "    indices = list(W[i].keys())\n",
    "\n",
    "    data = {\n",
    "        'Train_Data_Size': [train_size] * len(indices),\n",
    "        'Transformation': [type if type != \"\" else None] * len(indices),\n",
    "        'W': [W[i][j] for j in indices],\n",
    "        'b': [b[i][j] for j in indices],\n",
    "        'Accuracy': [correct[i][j] for j in indices],\n",
    "        'Co_Sim_CLS': [sim_cls[i][j] for j in indices],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data, index=indices)\n",
    "\n",
    "    name = f\"{type}_Label{i}_{train_size}_Augmentation_Results.csv\"\n",
    "    path = os.path.join(sub, name)\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
