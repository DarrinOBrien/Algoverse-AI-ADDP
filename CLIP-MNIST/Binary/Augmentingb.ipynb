{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "Author: Darrin O'Brien, email darrinobrien5@gmail.com\n",
    "1. Preparation\n",
    "2. Loads Base and Fine-Tuned on MNIST CLIP Models\n",
    "3. Seperates Test Set into 10 different subsets, each of the number {1,2,3...9}. So one testset is only comprised of a single label, e.g. label 0.\n",
    "3. Extracts Image Embedding Vectors of both models on test sets. From base layers {1,2,...5} to fine-tuned last layer's transformer layer 12. \n",
    "4. Applys learned translation vector b term to augment base CLIP in 5 different ways.  \n",
    "5. Evaluates the performance of the augmented models in comparison to the base and fine-tuned models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Installs for Runpod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up Device + Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device) # https://github.com/openai/CLIP\n",
    "clip_model = clip_model.float().to(device) # For fp-32 precision\n",
    "mnist = load_dataset(\"ylecun/mnist\") # https://huggingface.co/datasets/ylecun/mnist\n",
    "split = mnist[\"train\"].train_test_split(test_size=0.2, seed=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapper Class for Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module): # for fine-tuned model\n",
    "  def __init__(self, clip_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWithHooks(nn.Module):\n",
    "  def __init__(self, clip_model, classifier_head):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.cls_tokens = []\n",
    "    self.classifier = classifier_head\n",
    "\n",
    "  def forward(self, images):\n",
    "    self.cls_tokens = []\n",
    "\n",
    "    # B is batch\n",
    "    x = self.clip.visual.conv1(images)  # Convert image into patch embeddings. Divided into 32*32 patches. Shape is [B, 768, 7, 7]. Each 32*32 batch becomes a 768 dimensional vector. For 224*224 input, get 7*7=49 patches. Now have 49 such vectors per image.\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # -> [B, 768, 49] -> [B, 49, 768]; Each image is a sequence of 49 token vectors each of size 768, ready for the transformer.\n",
    "    x = x.permute(0,2,1)\n",
    "\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype) # Adds positional information so transformer knows order and position. [B, 50, 768] + [1, 50, 768]\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize to stablize it\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, 64, 768]\n",
    "\n",
    "    # Run resblocks manually, so hooks definitely trigger\n",
    "    for i, resblock in enumerate(self.clip.visual.transformer.resblocks):\n",
    "        x = resblock(x)\n",
    "        self.cls_tokens.append(x[0, :, :].detach())\n",
    "\n",
    "    x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None: # Linear Projection from 768 CLS token to 512 dimension vector for compatability\n",
    "      final_embed = x @ self.clip.visual.proj\n",
    "      final_embed = final_embed.detach()\n",
    "    else:\n",
    "      final_embed = x\n",
    "      final_embed = final_embed.detach()\n",
    "    \n",
    "    logits = self.classifier(final_embed)\n",
    "\n",
    "    return {\n",
    "      \"first_cls\": self.cls_tokens[0],\n",
    "      \"second_cls\": self.cls_tokens[1],\n",
    "      \"third_cls\": self.cls_tokens[2],\n",
    "      \"fourth_cls\": self.cls_tokens[3],\n",
    "      \"fifth_cls\": self.cls_tokens[4],\n",
    "      \"sixth_cls\": self.cls_tokens[5],\n",
    "      \"seventh_cls\": self.cls_tokens[6],\n",
    "      \"eighth_cls\": self.cls_tokens[7],\n",
    "      \"ninth_cls\": self.cls_tokens[8],\n",
    "      \"tenth_cls\": self.cls_tokens[9],\n",
    "      \"eleventh_cls\": self.cls_tokens[10],\n",
    "      \"twelfth_cls\": self.cls_tokens[11],\n",
    "      \"final_embed\": final_embed,\n",
    "      \"logits\": logits,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float().to(device)\n",
    "\n",
    "base = CLIPWithHooks(copy.deepcopy(refer), nn.Linear(refer.visual.output_dim, 2)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval().to(device)\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = CLIPWithHooks(f_t.clip, classifier_head=f_t.classifier) # Load in fine-tuned model with fine-tuned visual encoder. Basically just the fine-tuned model's visual and text encoder.\n",
    "fine_tuned = fine_tuned.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for item in batch:\n",
    "    img = item[\"image\"].convert(\"RGB\")  # Already a PIL Image\n",
    "    img = preprocess(img)\n",
    "    images.append(img)\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  return {\n",
    "      \"pixel_values\": images.to(device),\n",
    "      \"labels\": labels.to(device)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting up Individual Label Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = {}\n",
    "val_loader = {}\n",
    "test_loader = {}\n",
    "\n",
    "# 0-9\n",
    "for i in range(10):\n",
    "    train_dataset = split[\"train\"].filter(lambda example: example[\"label\"] == i)\n",
    "    val_dataset = split[\"test\"].filter(lambda example: example[\"label\"] == i)\n",
    "    test_dataset = mnist[\"test\"].filter(lambda example: example[\"label\"] == i)\n",
    "\n",
    "    # Making the training dataset for learned transformation/translation smaller. \n",
    "    '''\n",
    "    train_dataset = train_dataset.select(range(min(600, len(train_dataset))))\n",
    "    '''\n",
    "\n",
    "    train_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "    val_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "    test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "    train_loader[i] = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "    val_loader[i] = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "    test_loader[i] = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Translation Vector b\n",
    "def retrieve_avg(Z0, Z1):\n",
    "    Z0_arr = np.array(Z0)\n",
    "    Z1_arr = np.array(Z1)\n",
    "\n",
    "    diff = Z1_arr - Z0_arr\n",
    "\n",
    "    b = np.mean(diff, axis=0)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_first = {}\n",
    "b_second = {}\n",
    "b_third = {}\n",
    "b_fourth = {}\n",
    "b_fifth = {}\n",
    "\n",
    "# For Labels 0-9\n",
    "for i in range(10):\n",
    "    # Least Squares Regression\n",
    "    Z0_first_lsr = []\n",
    "    Z0_second_lsr = []\n",
    "    Z0_third_lsr = []\n",
    "    Z0_fourth_lsr = []\n",
    "    Z0_fifth_lsr = []\n",
    "\n",
    "    Z1_twelfth_lsr = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader[i], desc=f\"Extracting Train Set Label = {i} Vectors\"):\n",
    "            images = batch[\"pixel_values\"]\n",
    "\n",
    "            out_base = base(images)\n",
    "            out_fine_tuned = fine_tuned(images)\n",
    "\n",
    "            Z0_first_lsr.append(out_base[\"first_cls\"].float())\n",
    "            Z0_second_lsr.append(out_base[\"second_cls\"].float())\n",
    "            Z0_third_lsr.append(out_base[\"third_cls\"].float())\n",
    "            Z0_fourth_lsr.append(out_base[\"fourth_cls\"].float())\n",
    "            Z0_fifth_lsr.append(out_base[\"fifth_cls\"].float())\n",
    "\n",
    "            Z1_twelfth_lsr.append(out_fine_tuned[\"twelfth_cls\"].float())\n",
    "\n",
    "    Z0_first_lsr = torch.cat(Z0_first_lsr)\n",
    "    Z0_second_lsr = torch.cat(Z0_second_lsr)\n",
    "    Z0_third_lsr = torch.cat(Z0_third_lsr)\n",
    "    Z0_fourth_lsr = torch.cat(Z0_fourth_lsr)\n",
    "    Z0_fifth_lsr = torch.cat(Z0_fifth_lsr)\n",
    "\n",
    "    Z1_twelfth_lsr = torch.cat(Z1_twelfth_lsr)\n",
    "\n",
    "    Z0_first_lsr = Z0_first_lsr.cpu().numpy()\n",
    "    Z0_second_lsr = Z0_second_lsr.cpu().numpy()\n",
    "    Z0_third_lsr = Z0_third_lsr.cpu().numpy()\n",
    "    Z0_fourth_lsr = Z0_fourth_lsr.cpu().numpy()\n",
    "    Z0_fifth_lsr = Z0_fifth_lsr.cpu().numpy()\n",
    "\n",
    "    Z1_twelfth_lsr = Z1_twelfth_lsr.cpu().numpy()\n",
    "\n",
    "    b_first[i] = retrieve_avg(Z0_first_lsr, Z1_twelfth_lsr)\n",
    "    b_second[i] = retrieve_avg(Z0_second_lsr, Z1_twelfth_lsr)\n",
    "    b_third[i] = retrieve_avg(Z0_third_lsr, Z1_twelfth_lsr)\n",
    "    b_fourth[i] = retrieve_avg(Z0_fourth_lsr, Z1_twelfth_lsr)\n",
    "    b_fifth[i] = retrieve_avg(Z0_fifth_lsr, Z1_twelfth_lsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrapper Classes for Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedCLIP(nn.Module):\n",
    "    def __init__(self, clip, W=None, b=None, transform_stage=None, classifier=None):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.b = torch.from_numpy(b.astype(np.float32)).to(device) if b is not None else None\n",
    "        self.transform_stage = transform_stage if transform_stage is not None else -1\n",
    "        self.classifier = classifier if classifier is not None else nn.Linear(self.clip.visual.output_dim, 10)\n",
    "    \n",
    "    def forward(self, image): \n",
    "        image = image.to(device)\n",
    "        x = self.clip.visual.conv1(image)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "        x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "        x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "        x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, 64, 768]\n",
    "\n",
    "        if self.transform_stage == -1:\n",
    "            x = self.clip.visual.transformer(x)\n",
    "        else:\n",
    "            for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "                x = block(x)\n",
    "                if i+1 == self.transform_stage:\n",
    "                    cls = x[0, :, :]\n",
    "                    cls = cls.to(torch.float32)\n",
    "                    if self.W is None:\n",
    "                        self.W = torch.eye(cls.shape[-1], device=cls.device, dtype=cls.dtype)\n",
    "                    if self.b is None:\n",
    "                        self.b = torch.zeros(cls.shape[-1], device=cls.device, dtype=cls.dtype)\n",
    "                    manipulated = cls @ self.W + self.b\n",
    "                    break\n",
    "            manipulated = manipulated.unsqueeze(0) # Shape (1, B, D)\n",
    "            x = torch.cat([manipulated, x[1:, :, :]], dim=0) # Adds manipulated cls token all together, not seperately\n",
    "        twelfth_cls = x[0, :, :].squeeze()\n",
    "        \n",
    "        x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "        x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.clip.visual.proj is not None:\n",
    "            final_embed = x @ self.clip.visual.proj\n",
    "            final_embed = final_embed\n",
    "        else:\n",
    "            final_embed = x\n",
    "            final_embed = final_embed\n",
    "        \n",
    "        logits = self.classifier(final_embed)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"final_embed\": final_embed,\n",
    "            \"manipulated_cls\": manipulated.squeeze(0) if self.transform_stage != -1 else twelfth_cls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = AugmentedCLIP(f_t.clip, classifier=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval().to(device)\n",
    "\n",
    "base_classifier = nn.Linear(refer.visual.output_dim, 10)\n",
    "base = AugmentedCLIP(copy.deepcopy(refer), classifier=base_classifier)\n",
    "base = base.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_1 = {}\n",
    "aug_2 = {}\n",
    "aug_3 = {}\n",
    "aug_4 = {}\n",
    "aug_5 = {}\n",
    "\n",
    "# Creating Augmented Models for each of the labels\n",
    "for i in range(10):\n",
    "    model_1 = AugmentedCLIP(copy.deepcopy(refer), b=b_first[i], transform_stage=1, classifier=f_t.classifier)\n",
    "    model_1 = model_1.eval().to(device)\n",
    "    aug_1[i] = model_1 \n",
    "\n",
    "    model_2 = AugmentedCLIP(copy.deepcopy(refer), b=b_second[i], transform_stage=2, classifier=f_t.classifier)\n",
    "    model_2 = model_2.eval().to(device)\n",
    "    aug_2[i] = model_2 \n",
    "\n",
    "    model_3 = AugmentedCLIP(copy.deepcopy(refer), b=b_third[i], transform_stage=3, classifier=f_t.classifier)\n",
    "    model_3 = model_3.eval().to(device)\n",
    "    aug_3[i] = model_3 \n",
    "\n",
    "    model_4 = AugmentedCLIP(copy.deepcopy(refer), b=b_fourth[i], transform_stage=4, classifier=f_t.classifier)\n",
    "    model_4 = model_4.eval().to(device)\n",
    "    aug_4[i] = model_4\n",
    "\n",
    "    model_5 = AugmentedCLIP(copy.deepcopy(refer), b=b_fifth[i], transform_stage=5, classifier=f_t.classifier)\n",
    "    model_5 = model_5.eval().to(device)\n",
    "    aug_5[i] = model_5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating Augmented Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPred(model, images):\n",
    "    logits = model(images)[\"logits\"]\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return pred\n",
    "\n",
    "def cosineSimilarity(aug, fine, images):\n",
    "    out_aug = aug(images)\n",
    "    out_aug_embed = out_aug[\"final_embed\"]\n",
    "    out_aug_cls = out_aug[\"manipulated_cls\"]\n",
    "\n",
    "    out_fine = fine(images)\n",
    "    out_fine_embed = out_fine[\"final_embed\"]\n",
    "    out_fine_cls = out_fine[\"manipulated_cls\"]\n",
    "\n",
    "    # Prevent NaNs\n",
    "    eps = 1e-8\n",
    "    out_aug_embed = F.normalize(out_aug_embed, dim=1, eps=eps)\n",
    "    out_fine_embed = F.normalize(out_fine_embed, dim=1, eps=eps)\n",
    "    out_aug_cls = F.normalize(out_aug_cls, dim=1, eps=eps)\n",
    "    out_fine_cls = F.normalize(out_fine_cls, dim=1, eps=eps)\n",
    "\n",
    "    cos_sim_final = (out_aug_embed * out_fine_embed).sum(dim=1).mean().item()\n",
    "    cos_sim_cls = (out_aug_cls * out_fine_cls).sum(dim=1).mean().item()\n",
    "    return cos_sim_final, cos_sim_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_1 = {}\n",
    "accuracy_2 = {}\n",
    "accuracy_3 = {}\n",
    "accuracy_4 = {}\n",
    "accuracy_5 = {}\n",
    "\n",
    "accuracy_base = {}\n",
    "accuracy_fine_tuned = {}\n",
    "\n",
    "co_sim_final_1 = {}\n",
    "co_sim_cls_1 = {}\n",
    "\n",
    "co_sim_final_2 = {}\n",
    "co_sim_cls_2 = {}\n",
    "\n",
    "co_sim_final_3 = {}\n",
    "co_sim_cls_3 = {}\n",
    "\n",
    "co_sim_final_4 = {}\n",
    "co_sim_cls_4 = {}\n",
    "\n",
    "co_sim_final_5 = {}\n",
    "co_sim_cls_5 = {}\n",
    "\n",
    "for i in range(10):\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    correct_3 = 0\n",
    "    correct_4 = 0\n",
    "    correct_5 = 0\n",
    "    correct_base = 0\n",
    "    correct_fine_tuned = 0\n",
    "\n",
    "    total_samples = 0\n",
    "\n",
    "    sim_final_1 = []\n",
    "    sim_cls_1 = []\n",
    "\n",
    "    sim_final_2 = []\n",
    "    sim_cls_2 = []\n",
    "\n",
    "    sim_final_3 = []\n",
    "    sim_cls_3 = []\n",
    "\n",
    "    sim_final_4 = []\n",
    "    sim_cls_4 = []\n",
    "\n",
    "    sim_final_5 = []\n",
    "    sim_cls_5 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader[i], desc=f\"Evaluating {i}'s\"):\n",
    "            images = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            correct_1 += (calcPred(aug_1[i], images) == labels).sum().item()\n",
    "            correct_2 += (calcPred(aug_2[i], images) == labels).sum().item()\n",
    "            correct_3 += (calcPred(aug_3[i], images) == labels).sum().item()\n",
    "            correct_4 += (calcPred(aug_4[i], images) == labels).sum().item()\n",
    "            correct_5 += (calcPred(aug_5[i], images) == labels).sum().item()\n",
    "\n",
    "            correct_base += (calcPred(base, images) == labels).sum().item()\n",
    "            correct_fine_tuned += (calcPred(fine_tuned, images) == labels).sum().item()\n",
    "\n",
    "            cs_final, cs_cls = cosineSimilarity(aug_1[i], fine_tuned, images)\n",
    "            sim_final_1.append(cs_final)\n",
    "            sim_cls_1.append(cs_cls)\n",
    "\n",
    "            cs_final, cs_cls = cosineSimilarity(aug_2[i], fine_tuned, images)\n",
    "            sim_final_2.append(cs_final)\n",
    "            sim_cls_2.append(cs_cls)\n",
    "\n",
    "            cs_final, cs_cls = cosineSimilarity(aug_3[i], fine_tuned, images)\n",
    "            sim_final_3.append(cs_final)\n",
    "            sim_cls_3.append(cs_cls)\n",
    "\n",
    "            cs_final, cs_cls = cosineSimilarity(aug_4[i], fine_tuned, images)\n",
    "            sim_final_4.append(cs_final)\n",
    "            sim_cls_4.append(cs_cls)\n",
    "\n",
    "            cs_final, cs_cls = cosineSimilarity(aug_5[i], fine_tuned, images)\n",
    "            sim_final_5.append(cs_final)\n",
    "            sim_cls_5.append(cs_cls)\n",
    "\n",
    "    accuracy_1[i] = correct_1  / total_samples\n",
    "    accuracy_2[i]= correct_2 / total_samples\n",
    "    accuracy_3[i] = correct_3 / total_samples\n",
    "    accuracy_4[i] = correct_4 / total_samples\n",
    "    accuracy_5[i] = correct_5 / total_samples\n",
    "\n",
    "    accuracy_base[i] = correct_base / total_samples\n",
    "    accuracy_fine_tuned[i] = correct_fine_tuned / total_samples\n",
    "\n",
    "    co_sim_final_1[i] = np.mean(sim_final_1)\n",
    "    co_sim_cls_1[i] = np.mean(sim_cls_1)\n",
    "\n",
    "    co_sim_final_2[i] = np.mean(sim_final_2)\n",
    "    co_sim_cls_2[i] = np.mean(sim_cls_2)\n",
    "\n",
    "    co_sim_final_3[i] = np.mean(sim_final_3)\n",
    "    co_sim_cls_3[i] = np.mean(sim_cls_3)\n",
    "\n",
    "    co_sim_final_4[i] = np.mean(sim_final_4)\n",
    "    co_sim_cls_4[i] = np.mean(sim_cls_4)\n",
    "\n",
    "    co_sim_final_5[i] = np.mean(sim_final_5)\n",
    "    co_sim_cls_5[i] = np.mean(sim_cls_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Augmented CLIP on MNIST Label {i} Results\")\n",
    "    print(f\"\\tAugmented 1st - Last Layer Accuracy: {accuracy_1[i]:.4f}\")\n",
    "    print(f\"\\tAugmented 2nd - Last Layer Accuracy: {accuracy_2[i]:.4f}\")\n",
    "    print(f\"\\tAugmented 3rd - Last Layer Accuracy: {accuracy_3[i]:.4f}\")\n",
    "    print(f\"\\tAugmented 4th - Last Layer Accuracy: {accuracy_4[i]:.4f}\")\n",
    "    print(f\"\\tAugmented 5th - Last Layer Accuracy: {accuracy_5[i]:.4f}\")\n",
    "    print(f\"\\tBase Accuracy: {accuracy_base[i]:.4f}\")\n",
    "    print(f\"\\tFine-Tuned Accuracy: {accuracy_fine_tuned[i]:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"\\tAverage Cosine Similarity for final embedding of Augmented 1st Layer: {co_sim_final_1[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for final embedding of Augmented 2nd Layer: {co_sim_final_2[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for final embedding of Augmented 3rd Layer: {co_sim_final_3[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for final embedding of Augmented 4th Layer: {co_sim_final_4[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for final embedding of Augmented 5th Layer: {co_sim_final_5[i]:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"\\tAverage Cosine Similarity for CLS token of Augmented 1st Layer: {co_sim_cls_1[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for CLS token of Augmented 2nd Layer: {co_sim_cls_2[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for CLS token of Augmented 3rd Layer: {co_sim_cls_3[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for CLS token of Augmented 4th Layer: {co_sim_cls_4[i]:.4f}\")\n",
    "    print(f\"\\tAverage Cosine Similarity for CLS token of Augmented 5th Layer: {co_sim_cls_5[i]:.4f}\")\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
