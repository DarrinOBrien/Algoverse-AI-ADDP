{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "Author: Darrin O'Brien, email darrinobrien5@gmail.com\n",
    "\n",
    "1. Preparation.\n",
    "2. Loads Base and Fine-Tuned on MNIST CLIP Models.\n",
    "3. Loads and formats entire MNIST dataset. Option to minimize training dataset.\n",
    "4. Extracts Image Embedding Vectors of both models on the test sets. From base layers {1,2,..,5} to fine-tuned last layer's transformer layer 12.\n",
    "5. Applys learned transformation matrix W term to augment base CLIP in 5 different ways.\n",
    "6. Evaluates the performance of the augmented models in comparison to the base and fine-tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Runpod Installs Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up Device + Entire Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affine = False\n",
    "Transformation_Matrix = True\n",
    "Translation_Vector = False\n",
    "\n",
    "type = \"\"\n",
    "\n",
    "if Affine:\n",
    "    type = \"Affine_W_b\"\n",
    "elif Transformation_Matrix:\n",
    "    type = \"Transformation_Matrix_W\"\n",
    "elif Translation_Vector:\n",
    "    type = \"Translation_Vector_b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device) # https://github.com/openai/CLIP\n",
    "clip_model = clip_model.float().to(device) # For fp-32 precision\n",
    "mnist = load_dataset(\"ylecun/mnist\") # https://huggingface.co/datasets/ylecun/mnist\n",
    "split = mnist[\"train\"].train_test_split(test_size=0.2, seed=66)\n",
    "\n",
    "train_dataset = split[\"train\"] # 48,000 examples (direct training data from training set)\n",
    "val_dataset = split[\"test\"] # 12,000 examples (validation set split from training set)\n",
    "test_dataset = mnist[\"test\"] # 10,000 examples (direct test set)\n",
    "\n",
    "# Between 1-Testing Images. 1/10 of Test Image Set. MNIST - 1k -> \n",
    "size_nums = [i for i in range(1, 100, 5)]\n",
    "size_nums.insert(0, float('inf'))\n",
    "size_indice = 0\n",
    "train_size = 0\n",
    "\n",
    "# This minimizes the training dataset to learn the transformations/translations. \n",
    "labels = []\n",
    "for i in range(10):\n",
    "    ds = train_dataset.filter(lambda example: example[\"label\"] == i)\n",
    "    num = min(size_nums[size_indice], len(ds))\n",
    "    ds = ds.select(range(num))\n",
    "    train_size += num\n",
    "    labels.append(ds)\n",
    "\n",
    "train_dataset = concatenate_datasets(labels)\n",
    "\n",
    "train_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "val_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapper Class for Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module): # for fine-tuned model\n",
    "  def __init__(self, clip_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns All CLS Tokens + Final Embedding + Logits \n",
    "class CLIPWithHooks(nn.Module):\n",
    "  def __init__(self, clip_model, classifier_head):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.cls_tokens = []\n",
    "    self.classifier = classifier_head\n",
    "\n",
    "  def forward(self, images):\n",
    "    self.cls_tokens = []\n",
    "\n",
    "    # B is batch\n",
    "    x = self.clip.visual.conv1(images)  # Convert image into patch embeddings. Divided into 32*32 patches. Shape is [B, 768, 7, 7]. Each 32*32 batch becomes a 768 dimensional vector. For 224*224 input, get 7*7=49 patches. Now have 49 such vectors per image.\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # -> [B, 768, 49] -> [B, 49, 768]; Each image is a sequence of 49 token vectors each of size 768, ready for the transformer.\n",
    "    x = x.permute(0,2,1)\n",
    "\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype) # Adds positional information so transformer knows order and position. [B, 50, 768] + [1, 50, 768]\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize to stablize it\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, 64, 768]\n",
    "\n",
    "    # Run resblocks manually, so hooks definitely trigger\n",
    "    for i, resblock in enumerate(self.clip.visual.transformer.resblocks):\n",
    "        x = resblock(x)\n",
    "        self.cls_tokens.append(x[0, :, :].detach())\n",
    "\n",
    "    x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None: # Linear Projection from 768 CLS token to 512 dimension vector for compatability\n",
    "      final_embed = x @ self.clip.visual.proj\n",
    "      final_embed = final_embed.detach()\n",
    "    else:\n",
    "      final_embed = x\n",
    "      final_embed = final_embed.detach()\n",
    "    \n",
    "    logits = self.classifier(final_embed)\n",
    "\n",
    "    return {\n",
    "      \"cls\": [i for i in self.cls_tokens],\n",
    "      \"logits\": logits,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float().to(device)\n",
    "\n",
    "base = CLIPWithHooks(copy.deepcopy(refer), nn.Linear(refer.visual.output_dim, 2)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval().to(device)\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = CLIPWithHooks(f_t.clip, classifier_head=f_t.classifier) # Load in fine-tuned model with fine-tuned visual encoder\n",
    "fine_tuned = fine_tuned.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for item in batch:\n",
    "    img = item[\"image\"].convert(\"RGB\")  # Already a PIL Image\n",
    "    img = preprocess(img)\n",
    "    images.append(img)\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  return {\n",
    "      \"pixel_values\": images.to(device),\n",
    "      \"labels\": labels.to(device)\n",
    "  }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extracting Image Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Least Squares\n",
    "def leastSquares(Z0, Z1):\n",
    "    W_full, residuals, rank, s = np.linalg.lstsq(Z0, Z1, rcond=None)\n",
    "    return W_full  \n",
    "\n",
    "def addBiasColumn(Z0):\n",
    "    ones = np.ones((Z0.shape[0], 1)) # (1,1)\n",
    "    return np.hstack([Z0, ones])\n",
    "\n",
    "def retrieve_avg(Z0, Z1):\n",
    "    if Affine:\n",
    "        Z0 = addBiasColumn(Z0)\n",
    "        full = leastSquares(Z0, Z1)\n",
    "        W = full[:-1]\n",
    "        b = full[-1]\n",
    "        return W, b\n",
    "    if Transformation_Matrix:\n",
    "        return leastSquares(Z0, Z1), None\n",
    "    if Translation_Vector:\n",
    "        Z0 = np.array(Z0)\n",
    "        Z1 = np.array(Z1)\n",
    "        diff = Z1 - Z0\n",
    "        return np.mean(diff, axis=0), None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0 = {}\n",
    "for i in [0,2,4,6,7,8,10,11]: \n",
    "    Z0[i] = []\n",
    "\n",
    "Z1_twelfth_lsr = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader, desc=f\"Extracting Train Dataset Vectors\"):\n",
    "        images = batch[\"pixel_values\"]\n",
    "\n",
    "        out_base = base(images)\n",
    "        out_fine_tuned = fine_tuned(images)\n",
    "        \n",
    "        for i in [0,2,4,6,7,8,10,11]:\n",
    "            Z0[i].append(out_base[\"cls\"][i].float().cpu())\n",
    "\n",
    "        Z1_twelfth_lsr.append(out_fine_tuned[\"cls\"][11].float().cpu())\n",
    "\n",
    "Z1_twelfth_lsr = torch.cat(Z1_twelfth_lsr)\n",
    "Z1_twelfth_lsr = Z1_twelfth_lsr.cpu().numpy()\n",
    "\n",
    "W = {}\n",
    "b = {}\n",
    "\n",
    "for key, value in Z0.items():\n",
    "    value = torch.cat(value)\n",
    "    value = value.cpu().numpy()\n",
    "    W[key], b[key] = retrieve_avg(value, Z1_twelfth_lsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrapper Classes for Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedCLIP(nn.Module):\n",
    "    def __init__(self, clip, W=None, b=None, transform_stage=None, classifier=None):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.b = torch.from_numpy(b.astype(np.float32)).to(device) if b is not None else None\n",
    "        self.transform_stage = transform_stage if transform_stage is not None else -1\n",
    "        self.classifier = classifier if classifier is not None else nn.Linear(self.clip.visual.output_dim, 10)\n",
    "    \n",
    "    def forward(self, image): \n",
    "        image = image.to(device)\n",
    "        x = self.clip.visual.conv1(image)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "        x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "        x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "        x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, 64, 768]\n",
    "\n",
    "        if self.transform_stage == -1:\n",
    "            x = self.clip.visual.transformer(x)\n",
    "        else:\n",
    "            for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "                x = block(x)\n",
    "                if i == self.transform_stage:\n",
    "                    cls = x[0, :, :]\n",
    "                    cls = cls.to(torch.float32)\n",
    "                    if self.W is None:\n",
    "                        self.W = torch.eye(cls.shape[-1], device=cls.device, dtype=cls.dtype)\n",
    "                    if self.b is None:\n",
    "                        self.b = torch.zeros(cls.shape[-1], device=cls.device, dtype=cls.dtype)\n",
    "                    manipulated = cls @ self.W + self.b\n",
    "                    break\n",
    "            manipulated = manipulated.unsqueeze(0) # Shape (1, B, D)\n",
    "            x = torch.cat([manipulated, x[1:, :, :]], dim=0) # Adds manipulated cls token all together, not seperately\n",
    "        \n",
    "        twelfth_cls = x[0, :, :].squeeze()\n",
    "        \n",
    "        x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "        x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.clip.visual.proj is not None:\n",
    "            final_embed = x @ self.clip.visual.proj\n",
    "        else:\n",
    "            final_embed = x\n",
    "        \n",
    "        logits = self.classifier(final_embed)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"manipulated_cls\": manipulated.squeeze(0) if self.transform_stage != -1 else twelfth_cls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32.pt\"))\n",
    "fine_tuned = AugmentedCLIP(f_t.clip, classifier=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval().to(device)\n",
    "\n",
    "base_classifier = nn.Linear(refer.visual.output_dim, 10)\n",
    "base = AugmentedCLIP(copy.deepcopy(refer), classifier=base_classifier)\n",
    "base = base.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = {}\n",
    "\n",
    "# 6 augmented models {0,2,4,6,7,8,10,11}\n",
    "for i in [0,2,4,6,7,8,10,11]: \n",
    "    model = AugmentedCLIP(copy.deepcopy(refer), W=W[i], b=b[i], transform_stage=i,classifier=f_t.classifier)\n",
    "    model = model.eval().to(device)\n",
    "    aug[i] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating Augmented Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPred(model, images):\n",
    "    logits = model(images)[\"logits\"]\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return pred\n",
    "\n",
    "def cosineSimilarity(aug, fine, images):\n",
    "    out_aug = aug(images)\n",
    "    out_aug_cls = out_aug[\"manipulated_cls\"]\n",
    "\n",
    "    out_fine = fine(images)\n",
    "    out_fine_cls = out_fine[\"manipulated_cls\"] # Not Actually manipulated. Just changed. \n",
    "\n",
    "    # Prevent NaNs\n",
    "    eps = 1e-8\n",
    "    out_aug_cls = F.normalize(out_aug_cls, dim=1, eps=eps)\n",
    "    out_fine_cls = F.normalize(out_fine_cls, dim=1, eps=eps)\n",
    "\n",
    "    cos_sim_cls = (out_aug_cls * out_fine_cls).sum(dim=1).mean().item()\n",
    "    return cos_sim_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "total_samples = 0\n",
    "\n",
    "correct = {}\n",
    "sim_cls = {}\n",
    "\n",
    "for i in [0,2,4,6,7,8,10,11]:\n",
    "    correct[i] = 0\n",
    "    sim_cls[i] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=f\"Evaluating\"):\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        correct_base += (calcPred(base, images) == labels).sum().item()\n",
    "        correct_fine_tuned += (calcPred(fine_tuned, images) == labels).sum().item()\n",
    "\n",
    "        for i in [0,2,4,6,7,8,10,11]:\n",
    "            correct[i] += (calcPred(aug[i], images) == labels).sum().item()\n",
    "            sim_cls[i].append(cosineSimilarity(aug[i], fine_tuned, images))\n",
    "\n",
    "for i in [0,2,4,6,7,8,10,11]:\n",
    "    correct[i] = correct[i] / total_samples\n",
    "    sim_cls[i] = np.mean(sim_cls[i])\n",
    "\n",
    "correct_base = correct_base / total_samples\n",
    "correct_fine_tuned = correct_fine_tuned / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Augmented CLIP on Entire MNIST Results\")\n",
    "for i in [0,2,4,6,7,8,10,11]:\n",
    "    print(f\"\\tAugmented {i+1} - Last (12) Layer Accuracy: {correct[i]}\")\n",
    "    print(f\"\\tAverage Cosine Similarity of CLS Token of Augmented {i+1} Layer: {sim_cls[i]:.4f}\")\n",
    "print(f\"Base Accuracy: {correct_base:.4f}\")\n",
    "print(f\"Fine-Tuned Accuracy: {correct_fine_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = f\"./Entire_{type}\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "indices = list(W.keys())\n",
    "\n",
    "data = {\n",
    "    'Train_Data_Size': [train_size]*len(indices),\n",
    "    \"Transformation\": [type if type != \"\" else \"None\"] * len(indices),\n",
    "    'W': [W[i] for i in indices], # data.loc[0, \"W\"] -> First layer transformation\n",
    "    'b': [b[i] for i in indices],\n",
    "    'Accuracy': [correct[i] for i in indices], \n",
    "    \"Co_Sim_CLS\": [sim_cls[i] for i in indices],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=indices)\n",
    "\n",
    "name = f\"{type}_Entire_Size_{train_size}_Augmentation_Results.csv\"\n",
    "path = os.path.join(folder, name)\n",
    "df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
