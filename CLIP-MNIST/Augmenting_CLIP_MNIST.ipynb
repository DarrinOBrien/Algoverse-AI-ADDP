{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "\n",
    "Author: Darrin O'Brien, email: darrinobrien5@gmail.com\n",
    "\n",
    "1. Preparation\n",
    "2. Loads Base and Fine-Tuned on MNIST CLIP Models\n",
    "3. Extracts Image Embedding Vectors of both models on test set. The vectors calculated comprise of the base and fine-tuned models first, last, and final embedding. \n",
    "4. Applys learned weight and bias terms to augment base CLIP in 6 different ways.  \n",
    "5. Evaluates the performance of the augmented models in comparison to the base and fine-tuned models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqn56BjwxmpC"
   },
   "source": [
    "## 1. Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "s0KBmxzpxr_b",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6m2gLHgxtwr"
   },
   "source": [
    "### 1 Possible Installs - Runpod only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9n4eQetDxygY",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LCj6hVuwx0wA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubqd8AN4x2mG"
   },
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GVh555jx49v"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFxE1baHx7-2"
   },
   "source": [
    "## 3. Setting up Device + Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRu53aq6yAQl"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mnist = load_dataset(\"ylecun/mnist\") # https://huggingface.co/datasets/ylecun/mnist\n",
    "train_dataset = mnist[\"train\"] # 60,000 Samples\n",
    "test_dataset = mnist[\"test\"] # 10,000 examples (direct test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDHzyh4uyPWy"
   },
   "outputs": [],
   "source": [
    "train_dataset.sest_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsfkR9UzyfaW"
   },
   "source": [
    "## 4. Wrapping Models & Prepping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yg2vkE7Lyiim"
   },
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "  def __init__(self, clip_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T93JWP3feFLM"
   },
   "outputs": [],
   "source": [
    "class CLIPWithHooks(nn.Module):\n",
    "  def __init__(self, clip_model, classifier_head):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.cls_tokens = []\n",
    "    self.classifier = classifier_head\n",
    "\n",
    "  def forward(self, images):\n",
    "    self.cls_tokens = []\n",
    "\n",
    "    # B is batch\n",
    "    x = self.clip.visual.conv1(images)  # Convert image into patch embeddings. Divided into 32*32 patches. Shape is [B, 768, 7, 7]. Each 32*32 batch becomes a 768 dimensional vector. For 224*224 input, get 7*7=49 patches. Now have 49 such vectors per image.\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # -> [B, 768, 49] -> [B, 49, 768]; Each image is a sequence of 49 token vectors each of size 768, ready for the transformer.\n",
    "    x = x.permute(0,2,1)\n",
    "\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype) # Adds positional information so transformer knows order and position. [B, 50, 768] + [1, 50, 768]\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize to stablize it\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, 64, 768]\n",
    "\n",
    "    # Run resblocks manually, so hooks definitely trigger\n",
    "    for i, resblock in enumerate(self.clip.visual.transformer.resblocks):\n",
    "        x = resblock(x)\n",
    "        self.cls_tokens.append(x[0, :, :].detach())\n",
    "\n",
    "    x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None: # Linear Projection from 768 CLS token to 512 dimension vector for compatability\n",
    "      final_embed = x @ self.clip.visual.proj\n",
    "      final_embed = final_embed.detach()\n",
    "    else:\n",
    "      final_embed = x\n",
    "      final_embed = final_embed.detach()\n",
    "    \n",
    "    logits = self.classifier(final_embed)\n",
    "\n",
    "    return {\n",
    "      \"first_cls\": self.cls_tokens[0],\n",
    "      \"second_cls\": self.cls_tokens[1],\n",
    "      \"third_cls\": self.cls_tokens[2],\n",
    "      \"fourth_cls\": self.cls_tokens[3],\n",
    "      \"fifth_cls\": self.cls_tokens[4],\n",
    "      \"sixth_cls\": self.cls_tokens[5],\n",
    "      \"seventh_cls\": self.cls_tokens[6],\n",
    "      \"eighth_cls\": self.cls_tokens[7],\n",
    "      \"ninth_cls\": self.cls_tokens[8],\n",
    "      \"tenth_cls\": self.cls_tokens[9],\n",
    "      \"eleventh_cls\": self.cls_tokens[10],\n",
    "      \"twelfth_cls\": self.cls_tokens[11],\n",
    "      \"final_embed\": final_embed,\n",
    "      \"logits\": logits,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVrwbgSPeqrq"
   },
   "outputs": [],
   "source": [
    "refer, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "base = CLIPWithHooks(copy.deepcopy(refer), nn.Linear(refer.visual.output_dim, 2)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_0_fp32\"))\n",
    "fine_tuned = CLIPWithHooks(copy.deepcopy(refer), classifier_head=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCGogjkrjanK"
   },
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for item in batch:\n",
    "    img = item[\"image\"].convert(\"RGB\")  # Already a PIL Image\n",
    "    img = preprocess(img)\n",
    "    images.append(img)\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  return {\n",
    "      \"pixel_values\": images.to(device),\n",
    "      \"labels\": labels.to(device)\n",
    "  }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7kprpVbfMkq"
   },
   "outputs": [],
   "source": [
    "# Base Model Embeddings\n",
    "Z0_first = []\n",
    "Z0_second = []\n",
    "Z0_third = []\n",
    "Z0_fourth = []\n",
    "Z0_fifth = []\n",
    "Z0_sixth = []\n",
    "Z0_seventh = []\n",
    "Z0_eight = []\n",
    "Z0_ninth = []\n",
    "Z0_tenth = []\n",
    "Z0_eleventh = []\n",
    "Z0_twelfth = []\n",
    "\n",
    "# Fine-Tuned Model Embeddings\n",
    "# Z1_first = []\n",
    "# Z1_second = []\n",
    "# Z1_third = []\n",
    "# Z1_fourth = []\n",
    "# Z1_fifth = []\n",
    "# Z1_sixth = []\n",
    "# Z1_seventh = []\n",
    "# Z1_eight = []\n",
    "# Z1_ninth = []\n",
    "# Z1_tenth = []\n",
    "# Z1_eleventh = []\n",
    "Z1_twelfth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(tqdm(train_loader, desc=\"Extracting Train Set Vectors\")):\n",
    "    images = batch[\"pixel_values\"]\n",
    "\n",
    "    out_base = base(images)\n",
    "    out_fine_tuned = fine_tuned(images)\n",
    "\n",
    "    Z0_first.append(out_base[\"first_cls\"].float())\n",
    "    Z0_second.append(out_base[\"second_cls\"].float())\n",
    "    Z0_third.append(out_base[\"third_cls\"].float())\n",
    "    Z0_fourth.append(out_base[\"fourth_cls\"].float())\n",
    "    Z0_fifth.append(out_base[\"fifth_cls\"].float())\n",
    "    Z0_sixth.append(out_base[\"sixth_cls\"].float())\n",
    "    Z0_seventh.append(out_base[\"seventh_cls\"].float())\n",
    "    Z0_eight.append(out_base[\"eighth_cls\"].float())\n",
    "    Z0_ninth.append(out_base[\"ninth_cls\"].float())\n",
    "    Z0_tenth.append(out_base[\"tenth_cls\"].float())\n",
    "    Z0_eleventh.append(out_base[\"eleventh_cls\"].float())\n",
    "    Z0_twelfth.append(out_base[\"twelfth_cls\"].float())\n",
    "\n",
    "    # Z1_first.append(out_fine_tuned[\"first_cls\"].float())\n",
    "    # Z1_second.append(out_fine_tuned[\"second_cls\"].float())\n",
    "    # Z1_third.append(out_fine_tuned[\"third_cls\"].float())\n",
    "    # Z1_fourth.append(out_fine_tuned[\"fourth_cls\"].float())\n",
    "    # Z1_fifth.append(out_fine_tuned[\"fifth_cls\"].float())\n",
    "    # Z1_sixth.append(out_fine_tuned[\"sixth_cls\"].float())\n",
    "    # Z1_seventh.append(out_fine_tuned[\"seventh_cls\"].float())\n",
    "    # Z1_eight.append(out_fine_tuned[\"eighth_cls\"].float())\n",
    "    # Z1_ninth.append(out_fine_tuned[\"ninth_cls\"].float())\n",
    "    # Z1_tenth.append(out_fine_tuned[\"tenth_cls\"].float())\n",
    "    # Z1_eleventh.append(out_fine_tuned[\"eleventh_cls\"].float())\n",
    "    Z1_twelfth.append(out_fine_tuned[\"twelfth_cls\"].float())\n",
    "\n",
    "Z0_first = torch.cat(Z0_first)\n",
    "Z0_second = torch.cat(Z0_second)\n",
    "Z0_third = torch.cat(Z0_third)\n",
    "Z0_fourth = torch.cat(Z0_fourth)\n",
    "Z0_fifth = torch.cat(Z0_fifth)\n",
    "Z0_sixth = torch.cat(Z0_sixth)\n",
    "Z0_seventh = torch.cat(Z0_seventh)\n",
    "Z0_eight = torch.cat(Z0_eight)\n",
    "Z0_ninth = torch.cat(Z0_ninth)\n",
    "Z0_tenth = torch.cat(Z0_tenth)\n",
    "Z0_eleventh = torch.cat(Z0_eleventh)\n",
    "Z0_twelfth = torch.cat(Z0_twelfth)\n",
    "\n",
    "Z1_twelfth = torch.cat(Z1_twelfth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0eqKUWmghhv"
   },
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     \"base_first\": Z0_first,\n",
    "#     \"base_last\": Z0_last,\n",
    "#     \"base_final\": Z0_final,\n",
    "#     \"fine_tuned_first\": Z1_best_first,\n",
    "#     \"fine_tuned_last\": Z1_best_last,\n",
    "#     \"fine_tuned_final\": Z1_best_final\n",
    "# }, \"embedding_pairs.pt\")\n",
    "\n",
    "# Code to load in\n",
    "# pairs = torch.load(\"embedding_pairs.pt\", map_location=device)\n",
    "# Z0_first = pairs[\"base_first\"]\n",
    "# Z0_last = pairs[\"base_last\"]\n",
    "# Z0_final = pairs[\"base_final\"]\n",
    "# Z1_best_first = pairs[\"fine_tuned_first\"]\n",
    "# Z1_best_last = pairs[\"fine_tuned_last\"]\n",
    "# Z1_best_final = pairs[\"fine_tuned_final\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC73JnqmfS6c"
   },
   "source": [
    "## 5. Calculating Linear Transformation Matrix and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d52Cqj-fbFA"
   },
   "outputs": [],
   "source": [
    "Z0_first = Z0_first.cpu().numpy()\n",
    "Z0_second = Z0_second.cpu().numpy()\n",
    "Z0_third = Z0_third.cpu().numpy()\n",
    "Z0_fourth = Z0_fourth.cpu().numpy()\n",
    "Z0_fifth = Z0_fifth.cpu().numpy()\n",
    "Z0_sixth = Z0_sixth.cpu().numpy()\n",
    "Z0_seventh = Z0_seventh.cpu().numpy()\n",
    "Z0_eight = Z0_eight.cpu().numpy()\n",
    "Z0_ninth = Z0_ninth.cpu().numpy()\n",
    "Z0_tenth = Z0_tenth.cpu().numpy()\n",
    "Z0_eleventh = Z0_eleventh.cpu().numpy()\n",
    "Z0_twelfth = Z0_twelfth.cpu().numpy()\n",
    "\n",
    "Z1_twelfth = Z1_twelfth.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbIVuUenfc39"
   },
   "outputs": [],
   "source": [
    "# Add constant 1 column for bias term\n",
    "def addBiasColumn(Z0):\n",
    "    ones = np.ones((Z0.shape[0], 1)) # (1,1)\n",
    "    return np.hstack([Z0, ones])\n",
    "\n",
    "Z0_first  = addBiasColumn(Z0_first)\n",
    "Z0_second = addBiasColumn(Z0_second)\n",
    "Z0_third  = addBiasColumn(Z0_third)\n",
    "Z0_fourth = addBiasColumn(Z0_fourth)\n",
    "Z0_fifth  = addBiasColumn(Z0_fifth)\n",
    "Z0_sixth  = addBiasColumn(Z0_sixth)\n",
    "Z0_seventh  = addBiasColumn(Z0_seventh)\n",
    "Z0_eighth = addBiasColumn(Z0_eight)\n",
    "Z0_ninth = addBiasColumn(Z0_ninth)\n",
    "Z0_tenth = addBiasColumn(Z0_tenth)\n",
    "Z0_eleventh = addBiasColumn(Z0_eleventh)\n",
    "Z0_twelfth  = addBiasColumn(Z0_twelfth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fd98ngXpfkas"
   },
   "outputs": [],
   "source": [
    "# Calculate Bias b and Linear Transformation Matrix W\n",
    "def leastSquares(Z0, Z1):\n",
    "    W_full, residuals, rank, s = np.linalg.lstq(Z0, Z1, rcond=None)\n",
    "    return W_full  \n",
    "\n",
    "W = leastSquares(Z0_first, Z1_twelfth)\n",
    "W1 = W[:-1]\n",
    "b1 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_second, Z1_twelfth)\n",
    "W2 = W[:-1]\n",
    "b2 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_third, Z1_twelfth)\n",
    "W3 = W[:-1]\n",
    "b3 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_fourth, Z1_twelfth)\n",
    "W4 = W[:-1]\n",
    "b4 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_fifth, Z1_twelfth)\n",
    "W5 = W[:-1]\n",
    "b5 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_sixth, Z1_twelfth)\n",
    "W6 = W[:-1]\n",
    "b6 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_seventh, Z1_twelfth)\n",
    "W7 = W[:-1]\n",
    "b7 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_eight, Z1_twelfth)\n",
    "W8 = W[:-1]\n",
    "b8 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_ninth, Z1_twelfth)\n",
    "W9 = W[:-1]\n",
    "b9 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_tenth, Z1_twelfth)\n",
    "W10 = W[:-1]\n",
    "b10 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_eleventh, Z1_twelfth)\n",
    "W11 = W[:-1]\n",
    "b11 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_twelfth, Z1_twelfth)\n",
    "W12 = W[:-1]\n",
    "b12 = W[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "x812VR4KewOD",
    "outputId": "ee332298-41da-4f74-d221-131ccb622c7f"
   },
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     \"W_first_first\": W_0_1,\n",
    "#     \"b_first_first\": b_0_1,\n",
    "#     \"W_first_last\": W_1_1,\n",
    "#     \"b_first_last\": b_1_1,\n",
    "#     \"W_first_final\": W_2_1,\n",
    "#     \"b_first_final\": b_2_1,\n",
    "#     \"W_last_last\": W_3_1,\n",
    "#     \"b_last_last\": b_3_1,\n",
    "#     \"W_last_final\": W_4_1,\n",
    "#     \"b_last_final\": b_4_1,\n",
    "#     \"W_final_final\": W_5_1,\n",
    "#     \"b_final_final\": b_5_1\n",
    "# }, \"clip_mnist_transformations.pt\")\n",
    "\n",
    "# Code to Load:\n",
    "# vals = torch.load(\"clip_mnist_transformations.pt\")\n",
    "# W_0_1 = vals[\"W_first_first\"]\n",
    "# b_0_1 = vals[\"b_first_first\"]\n",
    "# W_1_1 = vals[\"W_first_last\"]\n",
    "# b_1_1 = vals[\"b_first_last\"]\n",
    "# W_2_1 = vals[\"W_first_final\"]\n",
    "# b_2_1 = vals[\"b_first_final\"]\n",
    "# W_3_1 = vals[\"W_last_last\"]\n",
    "# b_3_1 = vals[\"b_last_last\"]\n",
    "# W_4_1 = vals[\"W_last_final\"]\n",
    "# b_4_1 = vals[\"b_last_final\"]\n",
    "# W_5_1 = vals[\"W_final_final\"]\n",
    "# b_5_1 = vals[\"b_final_final\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-vRPjMyfmYq"
   },
   "source": [
    "## 6. Augmenting Base CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuned classifier head\n",
    "class AugmentedCLIP(nn.Module):\n",
    "  def __init__(self, clip_model, W=None, b=None, transform_stage=None, classifier=None):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "    self.b = torch.from_numpy(b.astype(np.float32)).to(device) if b is not None else None\n",
    "    self.transform_stage = transform_stage if transform_stage is not None else -1\n",
    "    self.classifier = classifier if classifier is not None else nn.Linear(self.clip.visual.output_dim, 2)\n",
    "\n",
    "  def forward(self, image):\n",
    "    image = image.to(device) # [B, 3, 224, 224]\n",
    "    x = self.clip.visual.conv1(image) # [B, 768, 7, 7]\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # [B, width, 49]\n",
    "    x = x.permute(0, 2, 1) # [B, 49, 768]\n",
    "\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "    x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, B, 768] # NLD -> LND\n",
    "    \n",
    "    if i == -1:\n",
    "      x = self.clip.visual.transformer(x)\n",
    "    else:\n",
    "      for i, block in enumerate(self.clip.visual.transformer.resblocks): # Compatible for 1-11 layers to #12\n",
    "        x = block(x)\n",
    "        if i+1 == self.transform_stage:\n",
    "          cls = x[0, :, :]\n",
    "          cls = cls.to(torch.float32)\n",
    "          manipulated = cls @ self.W + self.b \n",
    "          break \n",
    "        manipulated = manipulated.unsqueeze(0) # Shape (1, B, D) -> (1, 64, 768)\n",
    "        x = torch.cat([manipulated, x[1:, :, :]], dim=0) # Adds manipulated cls token all together, not seperately\n",
    "    \n",
    "    x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]; LND -> NLD\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None:\n",
    "      final_embed = x @ self.clip.visual.proj\n",
    "      final_embed = final_embed\n",
    "    else:\n",
    "      final_embed = x\n",
    "      final_embed = final_embed\n",
    "        \n",
    "    logits = self.classifier(final_embed)\n",
    "\n",
    "    return logits\n",
    "\n",
    "    # Don't need to do normalization from original code. Need to preserve scale information to accurately fit W, b. Keep the true structural nature\n",
    "    # Can normalize if evaluating cosine similarities between embeddings. Normalize both base and fine-tuned if doign so before computing similarity/dot product.\n",
    "    # Just doing classification so no need for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "refer, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "f_t = CLIPClassifier(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_fp32\"))\n",
    "fine_tuned = AugmentedCLIP(copy.deepcopy(refer), classifier=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval()\n",
    "\n",
    "base = AugmentedCLIP(copy.deepcopy(refer), classifier=f_t.classifier) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval()\n",
    "\n",
    "# Base First layer -> Fine-Tuned last layer (12)\n",
    "first_last = AugmentedCLIP(copy.deepcopy(refer), W1, b1, transform_stage=1, classifier=f_t.classifier)\n",
    "first_last.eval()\n",
    "\n",
    "# Base Second layer -> Fine-Tuned last layer (12)\n",
    "second_last = AugmentedCLIP(copy.deepcopy(refer), W2, b2, transform_stage=2, classifier=f_t.classifier)\n",
    "second_last.eval()\n",
    "\n",
    "# Base Third layer -> Fine-Tuned last layer (12)\n",
    "third_last = AugmentedCLIP(copy.deepcopy(refer), W3, b3, transform_stage=3, classifier=f_t.classifier)\n",
    "third_last.eval()\n",
    "\n",
    "# Base Fourth layer -> Fine-Tuned last layer (12)\n",
    "fourth_last = AugmentedCLIP(copy.deepcopy(refer), W4, b4, transform_stage=4, classifier=f_t.classifier)\n",
    "fourth_last.eval()\n",
    "\n",
    "# Base Fifth layer -> Fine-Tuned last layer (12)\n",
    "fifth_last = AugmentedCLIP(copy.deepcopy(refer), W5, b5, transform_stage=5, classifier=f_t.classifier)\n",
    "fifth_last.eval()\n",
    "\n",
    "# Base Sixth layer -> Fine-Tuned last layer (12)\n",
    "sixth_last = AugmentedCLIP(copy.deepcopy(refer), W6, b6, transform_stage=6, classifier=f_t.classifier)\n",
    "sixth_last.eval()\n",
    "\n",
    "# Base Seventh layer -> Fine-Tuned last layer (12)\n",
    "seventh_last = AugmentedCLIP(copy.deepcopy(refer), W7, b7, transform_stage=7, classifier=f_t.classifier)\n",
    "seventh_last.eval()\n",
    "\n",
    "# Base Eight layer -> Fine-Tuned last layer (12)\n",
    "eighth_last = AugmentedCLIP(copy.deepcopy(refer), W8, b8, transform_stage=8, classifier=f_t.classifier)\n",
    "eighth_last.eval()\n",
    "\n",
    "# Base Ninth layer -> Fine-Tuned last layer (12)\n",
    "ninth_last = AugmentedCLIP(copy.deepcopy(refer), W9, b9, transform_stage=9, classifier=f_t.classifier)\n",
    "ninth_last.eval()\n",
    "\n",
    "# Base Tenth layer -> Fine-Tuned last layer (12)\n",
    "tenth_last = AugmentedCLIP(copy.deepcopy(refer), W10, b10, transform_stage=10, classifier=f_t.classifier)\n",
    "tenth_last.eval()\n",
    "\n",
    "# Base Eleventh layer -> Fine-Tuned last layer (12)\n",
    "eleventh_last = AugmentedCLIP(copy.deepcopy(refer), W11, b11, transform_stage=11, classifier=f_t.classifier)\n",
    "eleventh_last.eval()\n",
    "\n",
    "# Base Twelfth layer -> Fine-Tuned last layer (12)\n",
    "twelfth_last = AugmentedCLIP(copy.deepcopy(refer), W12, b12, transform_stage=12, classifier=f_t.classifier)\n",
    "twelfth_last.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvqt0_xOs3cO"
   },
   "source": [
    "## 7. Evaluating Base v Fine-Tuned vs Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "correct_3 = 0\n",
    "correct_4 = 0\n",
    "correct_5 = 0\n",
    "correct_6 = 0\n",
    "correct_7 = 0\n",
    "correct_8 = 0\n",
    "correct_9 = 0\n",
    "correct_10 = 0\n",
    "correct_11 = 0\n",
    "correct_12 = 0\n",
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "def calcPred(model, images):\n",
    "    logits = model(images)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return pred\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        correct_1 += (calcPred(first_last, images) == labels).sum().item()\n",
    "        correct_2 += (calcPred(second_last, images) == labels).sum().item()\n",
    "        correct_3 += (calcPred(third_last, images) == labels).sum().item()\n",
    "        correct_4 += (calcPred(fourth_last, images) == labels).sum().item()\n",
    "        correct_5 += (calcPred(fifth_last, images) == labels).sum().item()\n",
    "        correct_6 += (calcPred(sixth_last, images) == labels).sum().item()\n",
    "        correct_7 += (calcPred(seventh_last, images) == labels).sum().item()\n",
    "        correct_8 += (calcPred(eighth_last, images) == labels).sum().item()\n",
    "        correct_9 += (calcPred(ninth_last, images) == labels).sum().item()\n",
    "        correct_10 += (calcPred(tenth_last, images) == labels).sum().item()\n",
    "        correct_11 += (calcPred(eleventh_last, images) == labels).sum().item()\n",
    "        correct_12 += (calcPred(twelfth_last, images) == labels).sum().item()\n",
    "\n",
    "        correct_base += (calcPred(base, images) == labels).sum().item()\n",
    "        correct_fine_tuned += (calcPred(fine_tuned, images) == labels).sum().item()\n",
    "\n",
    "acc_1 = correct_1 / total_samples\n",
    "acc_2 = correct_2 / total_samples\n",
    "acc_3 = correct_3 / total_samples\n",
    "acc_4 = correct_4 / total_samples\n",
    "acc_5 = correct_5 / total_samples\n",
    "acc_6 = correct_6 / total_samples\n",
    "acc_7 = correct_7 / total_samples\n",
    "acc_8 = correct_8 / total_samples\n",
    "acc_9 = correct_9 / total_samples\n",
    "acc_10 = correct_10 / total_samples\n",
    "acc_11 = correct_11 / total_samples\n",
    "acc_12 = correct_12 / total_samples\n",
    "\n",
    "acc_base = correct_base / total_samples\n",
    "acc_fine_tuned = correct_fine_tuned / total_samples\n",
    "\n",
    "print(f\"\\n\")\n",
    "for i, acc in enumerate([\n",
    "    acc_1, acc_2, acc_3, acc_4, acc_5, acc_6,\n",
    "    acc_7, acc_8, acc_9, acc_10, acc_11, acc_12\n",
    "]):\n",
    "    print(f\"Augmented {i+1} - Last Layer CLIP Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Hqn56BjwxmpC",
    "SC73JnqmfS6c",
    "f-vRPjMyfmYq",
    "Uvqt0_xOs3cO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
