{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "\n",
    "Author: Darrin O'Brien, email: darrinobrien5@gmail.com\n",
    "\n",
    "1. Preparation\n",
    "2. Load base and fine-tuned on MNIST for labels=0 CLIP ViT-32 Models\n",
    "2. Extracts Image Embedding Vectors of both models on train set. The vectors calculated comprise of the all the base model's transformer embedding to the fine-tuned models last embedding. \n",
    "3. Applys learned weight and bias terms to augment base CLIP in 12 different ways.  \n",
    "4. Evaluates the performance of the augmented models in comparison to the base and fine-tuned models on the MNIST test set of labels=0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Installs for Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import clip \n",
    "import numpy as np \n",
    "from datasets import load_dataset \n",
    "from tqdm import tqdm \n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model = clip_model.float()\n",
    "mnist = load_dataset(\"ylecun/mnist\")\n",
    "split = mnist[\"train\"].train_test_split(test_size=0.2, seed=66)\n",
    "\n",
    "train_dataset = split[\"train\"].filter(lambda example: example[\"label\"] == 0)\n",
    "val_dataset = split[\"test\"].filter(lambda example: example[\"label\"] == 0)\n",
    "test_dataset = mnist[\"test\"].filter(lambda example: example[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "val_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "\n",
    "    for item in batch:\n",
    "        img = preprocess(item[\"image\"])\n",
    "        images.append(img)\n",
    "        labels.append(item[\"label\"])\n",
    "    \n",
    "    images = torch.stack(images) # [Batch, channels, height, width] -> [64, 3, 224, 224]\n",
    "    labels = torch.tensor(labels, dtype=torch.long) # 64 bit integer, pytorch tensor\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": images.to(device),\n",
    "        \"labels\": labels.to(device)\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapper Class for CLIP for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier0(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes) # 512 -> 2\n",
    "    \n",
    "    def forward(self, images):\n",
    "        image_features = self.clip.encode_image(images)\n",
    "        logits = self.classifier(image_features)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Augmenting Base CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wrapper Models in Class for Extracting CLS Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPWithHooks(nn.Module):\n",
    "    def __init__(self, clip_model, classifier_head):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.cls_tokens = []\n",
    "        self.classifier = classifier_head \n",
    "    \n",
    "    def forward(self, images):\n",
    "        self.cls_tokens = []\n",
    "\n",
    "        x = self.clip.visual.conv1(images)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "        x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "        x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "        x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, 64, 768]\n",
    "\n",
    "        for block in self.clip.visual.transformer.resblocks:\n",
    "            x = block(x)\n",
    "            self.cls_tokens.append(x[0, :, :].detach())\n",
    "        \n",
    "        x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "        x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "        if self.clip.visual.proj is not None:\n",
    "            final_embed = x @ self.clip.visual.proj\n",
    "            final_embed = final_embed.detach()\n",
    "        else:\n",
    "            final_embed = x\n",
    "            final_embed = final_embed.detach()\n",
    "        \n",
    "        logits = self.classifier(final_embed)\n",
    "        \n",
    "        return {\n",
    "            \"first_cls\": self.cls_tokens[0],\n",
    "            \"second_cls\": self.cls_tokens[1],\n",
    "            \"third_cls\": self.cls_tokens[2],\n",
    "            \"fourth_cls\": self.cls_tokens[3],\n",
    "            \"fifth_cls\": self.cls_tokens[4],\n",
    "            \"sixth_cls\": self.cls_tokens[5],\n",
    "            \"seventh_cls\": self.cls_tokens[6],\n",
    "            \"eighth_cls\": self.cls_tokens[7],\n",
    "            \"ninth_cls\": self.cls_tokens[8],\n",
    "            \"tenth_cls\": self.cls_tokens[9],\n",
    "            \"eleventh_cls\": self.cls_tokens[10],\n",
    "            \"twelfth_cls\": self.cls_tokens[11],\n",
    "            \"final_embed\": final_embed,\n",
    "            \"logits\": logits,\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "base = base.float()\n",
    "base = CLIPWithHooks(base, nn.Linear(base.visual.output_dim, 2)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval()\n",
    "\n",
    "f_t, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "f_t = f_t.float()\n",
    "f_t = CLIPClassifier0(clip_model=f_t).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_0_fp32\"))\n",
    "fine_tuned = CLIPWithHooks(f_t.clip, classifier_head=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mnist[\"train\"].filter(lambda example: example[\"label\"] == 0)\n",
    "train.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting the Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model Embeddings\n",
    "Z0_first = []\n",
    "Z0_second = []\n",
    "Z0_third = []\n",
    "Z0_fourth = []\n",
    "Z0_fifth = []\n",
    "Z0_sixth = []\n",
    "Z0_seventh = []\n",
    "Z0_eight = []\n",
    "Z0_ninth = []\n",
    "Z0_tenth = []\n",
    "Z0_eleventh = []\n",
    "Z0_twelfth = []\n",
    "\n",
    "# Fine-Tuned Model Embeddings\n",
    "# Z1_first = []\n",
    "# Z1_second = []\n",
    "# Z1_third = []\n",
    "# Z1_fourth = []\n",
    "# Z1_fifth = []\n",
    "# Z1_sixth = []\n",
    "# Z1_seventh = []\n",
    "# Z1_eight = []\n",
    "# Z1_ninth = []\n",
    "# Z1_tenth = []\n",
    "# Z1_eleventh = []\n",
    "Z1_twelfth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=\"Extracting\")):\n",
    "        images = batch[\"pixel_values\"]\n",
    "\n",
    "        out_base = base(images)\n",
    "        out_fine_tuned = fine_tuned(images)\n",
    "\n",
    "        Z0_first.append(out_base[\"first_cls\"].float())\n",
    "        Z0_second.append(out_base[\"second_cls\"].float())\n",
    "        Z0_third.append(out_base[\"third_cls\"].float())\n",
    "        Z0_fourth.append(out_base[\"fourth_cls\"].float())\n",
    "        Z0_fifth.append(out_base[\"fifth_cls\"].float())\n",
    "        Z0_sixth.append(out_base[\"sixth_cls\"].float())\n",
    "        Z0_seventh.append(out_base[\"seventh_cls\"].float())\n",
    "        Z0_eight.append(out_base[\"eighth_cls\"].float())\n",
    "        Z0_ninth.append(out_base[\"ninth_cls\"].float())\n",
    "        Z0_tenth.append(out_base[\"tenth_cls\"].float())\n",
    "        Z0_eleventh.append(out_base[\"eleventh_cls\"].float())\n",
    "        Z0_twelfth.append(out_base[\"twelfth_cls\"].float())\n",
    "\n",
    "        # Z1_first.append(out_fine_tuned[\"first_cls\"].float())\n",
    "        # Z1_second.append(out_fine_tuned[\"second_cls\"].float())\n",
    "        # Z1_third.append(out_fine_tuned[\"third_cls\"].float())\n",
    "        # Z1_fourth.append(out_fine_tuned[\"fourth_cls\"].float())\n",
    "        # Z1_fifth.append(out_fine_tuned[\"fifth_cls\"].float())\n",
    "        # Z1_sixth.append(out_fine_tuned[\"sixth_cls\"].float())\n",
    "        # Z1_seventh.append(out_fine_tuned[\"seventh_cls\"].float())\n",
    "        # Z1_eight.append(out_fine_tuned[\"eighth_cls\"].float())\n",
    "        # Z1_ninth.append(out_fine_tuned[\"ninth_cls\"].float())\n",
    "        # Z1_tenth.append(out_fine_tuned[\"tenth_cls\"].float())\n",
    "        # Z1_eleventh.append(out_fine_tuned[\"eleventh_cls\"].float())\n",
    "        Z1_twelfth.append(out_fine_tuned[\"twelfth_cls\"].float())\n",
    "\n",
    "Z0_first = torch.cat(Z0_first)\n",
    "Z0_second = torch.cat(Z0_second)\n",
    "Z0_third = torch.cat(Z0_third)\n",
    "Z0_fourth = torch.cat(Z0_fourth)\n",
    "Z0_fifth = torch.cat(Z0_fifth)\n",
    "Z0_sixth = torch.cat(Z0_sixth)\n",
    "Z0_seventh = torch.cat(Z0_seventh)\n",
    "Z0_eight = torch.cat(Z0_eight)\n",
    "Z0_ninth = torch.cat(Z0_ninth)\n",
    "Z0_tenth = torch.cat(Z0_tenth)\n",
    "Z0_eleventh = torch.cat(Z0_eleventh)\n",
    "Z0_twelfth = torch.cat(Z0_twelfth)\n",
    "\n",
    "Z1_twelfth = torch.cat(Z1_twelfth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating Linear Transformation Matrixes and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z0_first = Z0_first.cpu().numpy()\n",
    "Z0_second = Z0_second.cpu().numpy()\n",
    "Z0_third = Z0_third.cpu().numpy()\n",
    "Z0_fourth = Z0_fourth.cpu().numpy()\n",
    "Z0_fifth = Z0_fifth.cpu().numpy()\n",
    "Z0_sixth = Z0_sixth.cpu().numpy()\n",
    "Z0_seventh = Z0_seventh.cpu().numpy()\n",
    "Z0_eight = Z0_eight.cpu().numpy()\n",
    "Z0_ninth = Z0_ninth.cpu().numpy()\n",
    "Z0_tenth = Z0_tenth.cpu().numpy()\n",
    "Z0_eleventh = Z0_eleventh.cpu().numpy()\n",
    "Z0_twelfth = Z0_twelfth.cpu().numpy()\n",
    "\n",
    "Z1_twelfth = Z1_twelfth.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addBiasColumn(Z0):\n",
    "    ones = np.ones((Z0.shape[0], 1)) # (1,1)\n",
    "    return np.hstack([Z0, ones])\n",
    "\n",
    "Z0_first  = addBiasColumn(Z0_first)\n",
    "Z0_second = addBiasColumn(Z0_second)\n",
    "Z0_third  = addBiasColumn(Z0_third)\n",
    "Z0_fourth = addBiasColumn(Z0_fourth)\n",
    "Z0_fifth  = addBiasColumn(Z0_fifth)\n",
    "Z0_sixth  = addBiasColumn(Z0_sixth)\n",
    "Z0_seventh  = addBiasColumn(Z0_seventh)\n",
    "Z0_eighth = addBiasColumn(Z0_eight)\n",
    "Z0_ninth = addBiasColumn(Z0_ninth)\n",
    "Z0_tenth = addBiasColumn(Z0_tenth)\n",
    "Z0_eleventh = addBiasColumn(Z0_eleventh)\n",
    "Z0_twelfth  = addBiasColumn(Z0_twelfth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leastSquares(Z0, Z1):\n",
    "    W_full, residuals, rank, s = np.linalg.lstq(Z0, Z1, rcond=None)\n",
    "    return W_full  \n",
    "\n",
    "W = leastSquares(Z0_first, Z1_twelfth)\n",
    "W1 = W[:-1]\n",
    "b1 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_second, Z1_twelfth)\n",
    "W2 = W[:-1]\n",
    "b2 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_third, Z1_twelfth)\n",
    "W3 = W[:-1]\n",
    "b3 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_fourth, Z1_twelfth)\n",
    "W4 = W[:-1]\n",
    "b4 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_fifth, Z1_twelfth)\n",
    "W5 = W[:-1]\n",
    "b5 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_sixth, Z1_twelfth)\n",
    "W6 = W[:-1]\n",
    "b6 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_seventh, Z1_twelfth)\n",
    "W7 = W[:-1]\n",
    "b7 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_eight, Z1_twelfth)\n",
    "W8 = W[:-1]\n",
    "b8 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_ninth, Z1_twelfth)\n",
    "W9 = W[:-1]\n",
    "b9 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_tenth, Z1_twelfth)\n",
    "W10 = W[:-1]\n",
    "b10 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_eleventh, Z1_twelfth)\n",
    "W11 = W[:-1]\n",
    "b11 = W[-1]\n",
    "\n",
    "W = leastSquares(Z0_twelfth, Z1_twelfth)\n",
    "W12 = W[:-1]\n",
    "b12 = W[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manipulating Base CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedCLIP(nn.Module):\n",
    "    def __init__(self, clip, W=None, b=None, transform_stage=None, classifier=None):\n",
    "        super().__init__()\n",
    "        self.clip = clip\n",
    "        self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.b = torch.from_numpy(b.astype(np.float32)).to(device) if W is not None else None\n",
    "        self.transform_stage = transform_stage if transform_stage is not None else -1\n",
    "        self.classifier = classifier if classifier is not None else nn.Linear(self.clip.visual.output_dim, 2)\n",
    "    \n",
    "    def forward(self, image): \n",
    "        image = image.to(device)\n",
    "        x = self.clip.visual.conv1(image)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # [B, Patchs+CLS (sequence_length), Embedding Dimension] -> [64, 50 (49+1), 768]\n",
    "        x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "        x = self.clip.visual.ln_pre(x) # Normalize for Stability\n",
    "\n",
    "        x = x.permute(1,0,2) # [sequence_length, batch_size, embedding_dim] -> [50, 64, 768]\n",
    "\n",
    "        if self.transform_stage == -1:\n",
    "            x = self.clip.visual.transformer(x)\n",
    "        else:\n",
    "            for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "                x = block(x)\n",
    "                if i+1 == self.transform_stage:\n",
    "                    cls = x[0, :, :]\n",
    "                    cls = cls.to(torch.float32)\n",
    "                    manipulated = cls @ self.W + self.b\n",
    "                    break\n",
    "            manipulated = manipulated.unsqueeze(0) # Shape (1, B, D)\n",
    "            x = torch.cat([manipulated, x[1:, :, :]], dim=0) # Adds manipulated cls token all together, not seperately\n",
    "\n",
    "        x = x.permute(1,0,2) # [batch_size, sequence_length, embedding_dim] -> [64, 50, 768]\n",
    "\n",
    "        x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.clip.visual.proj is not None:\n",
    "            final_embed = x @ self.clip.visual.proj\n",
    "            final_embed = final_embed\n",
    "        else:\n",
    "            final_embed = x\n",
    "            final_embed = final_embed\n",
    "        \n",
    "        logits = self.classifier(final_embed)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "refer = refer.float()\n",
    "\n",
    "base = AugmentedCLIP(copy.deepcopy(refer)) # Random Classifier Head. 50% chance of being right.\n",
    "base = base.eval()\n",
    "\n",
    "f_t = CLIPClassifier0(clip_model=copy.deepcopy(refer)).to(device) # Wrap in classifer to retrieve classifier head\n",
    "f_t.load_state_dict(torch.load(\"best_clip_mnist_0_fp32\"))\n",
    "fine_tuned = AugmentedCLIP(copy.deepcopy(refer), classifier=f_t.classifier) # Load in a raw CLIP model\n",
    "fine_tuned = fine_tuned.eval()\n",
    "\n",
    "# Base First layer -> Fine-Tuned last layer (12)\n",
    "first_last = AugmentedCLIP(copy.deepcopy(refer), W1, b1, transform_stage=1, classifier=f_t.classifier)\n",
    "first_last.eval()\n",
    "\n",
    "# Base Second layer -> Fine-Tuned last layer (12)\n",
    "second_last = AugmentedCLIP(copy.deepcopy(refer), W2, b2, transform_stage=2, classifier=f_t.classifier)\n",
    "second_last.eval()\n",
    "\n",
    "# Base Third layer -> Fine-Tuned last layer (12)\n",
    "third_last = AugmentedCLIP(copy.deepcopy(refer), W3, b3, transform_stage=3, classifier=f_t.classifier)\n",
    "third_last.eval()\n",
    "\n",
    "# Base Fourth layer -> Fine-Tuned last layer (12)\n",
    "fourth_last = AugmentedCLIP(copy.deepcopy(refer), W4, b4, transform_stage=4, classifier=f_t.classifier)\n",
    "fourth_last.eval()\n",
    "\n",
    "# Base Fifth layer -> Fine-Tuned last layer (12)\n",
    "fifth_last = AugmentedCLIP(copy.deepcopy(refer), W5, b5, transform_stage=5, classifier=f_t.classifier)\n",
    "fifth_last.eval()\n",
    "\n",
    "# Base Sixth layer -> Fine-Tuned last layer (12)\n",
    "sixth_last = AugmentedCLIP(copy.deepcopy(refer), W6, b6, transform_stage=6, classifier=f_t.classifier)\n",
    "sixth_last.eval()\n",
    "\n",
    "# Base Seventh layer -> Fine-Tuned last layer (12)\n",
    "seventh_last = AugmentedCLIP(copy.deepcopy(refer), W7, b7, transform_stage=7, classifier=f_t.classifier)\n",
    "seventh_last.eval()\n",
    "\n",
    "# Base Eight layer -> Fine-Tuned last layer (12)\n",
    "eighth_last = AugmentedCLIP(copy.deepcopy(refer), W8, b8, transform_stage=8, classifier=f_t.classifier)\n",
    "eighth_last.eval()\n",
    "\n",
    "# Base Ninth layer -> Fine-Tuned last layer (12)\n",
    "ninth_last = AugmentedCLIP(copy.deepcopy(refer), W9, b9, transform_stage=9, classifier=f_t.classifier)\n",
    "ninth_last.eval()\n",
    "\n",
    "# Base Tenth layer -> Fine-Tuned last layer (12)\n",
    "tenth_last = AugmentedCLIP(copy.deepcopy(refer), W10, b10, transform_stage=10, classifier=f_t.classifier)\n",
    "tenth_last.eval()\n",
    "\n",
    "# Base Eleventh layer -> Fine-Tuned last layer (12)\n",
    "eleventh_last = AugmentedCLIP(copy.deepcopy(refer), W11, b11, transform_stage=11, classifier=f_t.classifier)\n",
    "eleventh_last.eval()\n",
    "\n",
    "# Base Twelfth layer -> Fine-Tuned last layer (12)\n",
    "twelfth_last = AugmentedCLIP(copy.deepcopy(refer), W12, b12, transform_stage=12, classifier=f_t.classifier)\n",
    "twelfth_last.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Augmented Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_1 = 0\n",
    "correct_2 = 0\n",
    "correct_3 = 0\n",
    "correct_4 = 0\n",
    "correct_5 = 0\n",
    "correct_6 = 0\n",
    "correct_7 = 0\n",
    "correct_8 = 0\n",
    "correct_9 = 0\n",
    "correct_10 = 0\n",
    "correct_11 = 0\n",
    "correct_12 = 0\n",
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "def calcPred(model, images):\n",
    "    logits = model(images)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return pred\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        correct_1 += (calcPred(first_last, images) == labels).sum().item()\n",
    "        correct_2 += (calcPred(second_last, images) == labels).sum().item()\n",
    "        correct_3 += (calcPred(third_last, images) == labels).sum().item()\n",
    "        correct_4 += (calcPred(fourth_last, images) == labels).sum().item()\n",
    "        correct_5 += (calcPred(fifth_last, images) == labels).sum().item()\n",
    "        correct_6 += (calcPred(sixth_last, images) == labels).sum().item()\n",
    "        correct_7 += (calcPred(seventh_last, images) == labels).sum().item()\n",
    "        correct_8 += (calcPred(eighth_last, images) == labels).sum().item()\n",
    "        correct_9 += (calcPred(ninth_last, images) == labels).sum().item()\n",
    "        correct_10 += (calcPred(tenth_last, images) == labels).sum().item()\n",
    "        correct_11 += (calcPred(eleventh_last, images) == labels).sum().item()\n",
    "        correct_12 += (calcPred(twelfth_last, images) == labels).sum().item()\n",
    "\n",
    "        correct_base += (calcPred(base, images) == labels).sum().item()\n",
    "        correct_fine_tuned += (calcPred(fine_tuned, images) == labels).sum().item()\n",
    "\n",
    "acc_1 = correct_1 / total_samples\n",
    "acc_2 = correct_2 / total_samples\n",
    "acc_3 = correct_3 / total_samples\n",
    "acc_4 = correct_4 / total_samples\n",
    "acc_5 = correct_5 / total_samples\n",
    "acc_6 = correct_6 / total_samples\n",
    "acc_7 = correct_7 / total_samples\n",
    "acc_8 = correct_8 / total_samples\n",
    "acc_9 = correct_9 / total_samples\n",
    "acc_10 = correct_10 / total_samples\n",
    "acc_11 = correct_11 / total_samples\n",
    "acc_12 = correct_12 / total_samples\n",
    "\n",
    "acc_base = correct_base / total_samples\n",
    "acc_fine_tuned = correct_fine_tuned / total_samples\n",
    "\n",
    "print(f\"\\n\")\n",
    "for i, acc in enumerate([\n",
    "    acc_1, acc_2, acc_3, acc_4, acc_5, acc_6,\n",
    "    acc_7, acc_8, acc_9, acc_10, acc_11, acc_12\n",
    "]):\n",
    "    print(f\"Augmented {i+1} - Last Layer CLIP Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
