{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqn56BjwxmpC"
   },
   "source": [
    "##1. Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "s0KBmxzpxr_b",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6m2gLHgxtwr"
   },
   "source": [
    "1.1 Possible Installs - Runpod only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9n4eQetDxygY",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LCj6hVuwx0wA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubqd8AN4x2mG"
   },
   "source": [
    "##2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9GVh555jx49v"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFxE1baHx7-2"
   },
   "source": [
    "##3. Setting up Device + Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bRu53aq6yAQl"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mnist = load_dataset(\"ylecun/mnist\") # https://huggingface.co/datasets/ylecun/mnist\n",
    "test_dataset = mnist[\"test\"] # 10,000 examples (direct test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UDHzyh4uyPWy"
   },
   "outputs": [],
   "source": [
    "test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsfkR9UzyfaW"
   },
   "source": [
    "##4. Wrapping Models & Prepping Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Yg2vkE7Lyiim"
   },
   "outputs": [],
   "source": [
    "class CLIPClassifier(nn.Module):\n",
    "  def __init__(self, clip_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    image_features = self.clip.encode_image(images)\n",
    "    logits = self.classifier(image_features)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "dxPqRbvpyufc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (clip): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_CLIP, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = CLIPClassifier(clip_model=base_CLIP).to(device)\n",
    "model = model.float()\n",
    "\n",
    "best_CLIP, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "best_CLIP_MNIST = CLIPClassifier(clip_model=best_CLIP).to(device)\n",
    "best_CLIP_MNIST = best_CLIP_MNIST.float()\n",
    "best_CLIP_MNIST.load_state_dict(torch.load(\"best_clip_mnist.pt\", map_location=device)) # map_location tells where to place the model's weights in memory\n",
    "\n",
    "model.eval()\n",
    "best_CLIP_MNIST.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OCGogjkrjanK"
   },
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "  images = []\n",
    "  labels = []\n",
    "\n",
    "  for item in batch:\n",
    "    img = item[\"image\"].convert(\"RGB\")  # Already a PIL Image\n",
    "    img = preprocess(img)\n",
    "    images.append(img)\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "  return {\n",
    "      \"pixel_values\": images.to(device),\n",
    "      \"labels\": labels.to(device)\n",
    "  }\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T93JWP3feFLM"
   },
   "outputs": [],
   "source": [
    "class CLIPWithHooks(nn.Module):\n",
    "  def __init__(self, clip_model):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "\n",
    "    self.first_cls = None\n",
    "    self.last_cls = None\n",
    "\n",
    "    self.clip.visual.transformer.resblocks[0].register_forward_hook(self.save_first_cls)\n",
    "    self.clip.visual.transformer.resblocks[-1].register_forward_hook(self.save_last_cls)\n",
    "\n",
    "  def save_first_cls(self, module, input, output):\n",
    "    # input[0] is the full sequence: [batch, seq_len, dim]\n",
    "    self.first_cls = output[:, 0, :].detach() # CLS token\n",
    "\n",
    "  def save_last_cls(self, module, input, output):\n",
    "    self.last_cls = output[:, 0, :].detach() # CLS token\n",
    "\n",
    "  def forward(self, images):\n",
    "    self.first_cls = None\n",
    "    self.last_cls = None\n",
    "\n",
    "    # B is batch\n",
    "    x = self.clip.visual.conv1(images)  # Convert image into patch embeddings. Divided into 32*32 patches. Shape is [B, 768, 7, 7]. Each 32*32 batch becomes a 768 dimensional vector. For 224*224 input, get 7*7=49 patches. Now have 49 such vectors per image.\n",
    "    x = x.flatten(2).transpose(1,2) # -> [B, 768, 49] -> [B, 49, 768]; Each image is a sequence of 49 token vectors each of size 768, ready for the transformer.\n",
    "    x = torch.cat([self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # Adds learnable CLS token at the start of every image's token sequence. [1,768] -> [B,1,768] -> [B, 50, 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype) # Adds positional information so transformer knows order and position. [B, 50, 768] + [1, 50, 768]\n",
    "    x = self.clip.visual.ln_pre(x) # Normalize to stablize it\n",
    "\n",
    "    # Run resblocks manually, so hooks definitely trigger\n",
    "    for i, resblock in enumerate(self.clip.visual.transformer.resblocks):\n",
    "        x = resblock(x)\n",
    "        if i == 0: # First layer\n",
    "            self.first_cls = x[:, 0, :].detach()\n",
    "        if i == len(self.clip.visual.transformer.resblocks) - 1: # Last layer\n",
    "            self.last_cls = x[:, 0, :].detach()\n",
    "\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    final_embed = x @ self.clip.visual.proj # Linear Projection from 768 CLS token to 512 dimension vector for compatability\n",
    "\n",
    "    return {\n",
    "        \"first_cls\": self.first_cls,\n",
    "        \"last_cls\": self.last_cls,\n",
    "        \"final_embed\": final_embed.detach()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IVrwbgSPeqrq"
   },
   "outputs": [],
   "source": [
    "wrapped_base = CLIPWithHooks(base_CLIP)\n",
    "wrapped_best = CLIPWithHooks(best_CLIP_MNIST.clip)\n",
    "\n",
    "base_CLIP.eval()\n",
    "best_CLIP_MNIST.eval()\n",
    "\n",
    "def setNoGrad(model):\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "setNoGrad(base_CLIP)\n",
    "setNoGrad(best_CLIP_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dtypes: {torch.float32}\n",
      "Buffer    dtypes: set()\n",
      "Parameter dtypes: {torch.float32}\n",
      "Buffer    dtypes: set()\n"
     ]
    }
   ],
   "source": [
    "def print_module_dtypes(model): # Sanity Check for fp32\n",
    "    param_dtypes = {p.dtype for p in model.parameters()}\n",
    "    buffer_dtypes = {b.dtype for b in model.buffers()}\n",
    "    print(f\"Parameter dtypes: {param_dtypes}\")\n",
    "    print(f\"Buffer    dtypes: {buffer_dtypes}\")\n",
    "\n",
    "# Example usage:\n",
    "print_module_dtypes(wrapped_base)\n",
    "print_module_dtypes(wrapped_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s7kprpVbfMkq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 157/157 [01:54<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "Z0_first = [] # base model's first-layer embedding (CLS)\n",
    "Z0_last = [] # base model's last-layer embedding (CLS)\n",
    "Z0_final = [] # base model's final image embedding\n",
    "\n",
    "Z1_best_first = [] # best finetuned model's first-layer embedding (CLS)\n",
    "Z1_best_last = [] # best finetuned model's last-layer embedding (CLS)\n",
    "Z1_best_final = [] # best finetuned model's final image embedding\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i, batch in enumerate(tqdm(test_loader, desc=\"Extracting\")):\n",
    "    images = batch[\"pixel_values\"]\n",
    "      \n",
    "    out_base = wrapped_base(images)\n",
    "    out_best = wrapped_best(images)\n",
    "\n",
    "    Z0_first.append(out_base[\"first_cls\"].float())\n",
    "    Z0_last.append(out_base[\"last_cls\"].float())\n",
    "    Z0_final.append(out_base[\"final_embed\"].float())\n",
    "\n",
    "    Z1_best_first.append(out_best[\"first_cls\"].float())\n",
    "    Z1_best_last.append(out_best[\"last_cls\"].float())\n",
    "    Z1_best_final.append(out_best[\"final_embed\"].float())\n",
    "\n",
    "Z0_first = torch.cat(Z0_first)  # shape: [N, D]\n",
    "Z0_last = torch.cat(Z0_last)\n",
    "Z0_final = torch.cat(Z0_final)\n",
    "\n",
    "Z1_best_first = torch.cat(Z1_best_first)\n",
    "Z1_best_last = torch.cat(Z1_best_last)\n",
    "Z1_best_final = torch.cat(Z1_best_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "e0eqKUWmghhv"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"base_first\": Z0_first,\n",
    "    \"base_last\": Z0_last,\n",
    "    \"base_final\": Z0_final,\n",
    "    \"fine_tuned_first\": Z1_best_first,\n",
    "    \"fine_tuned_last\": Z1_best_last,\n",
    "    \"fine_tuned_final\": Z1_best_final\n",
    "}, \"embedding_pairs.pt\")\n",
    "\n",
    "# Code to load in\n",
    "# pairs = torch.load(\"embedding_pairs.pt\", map_location=device)\n",
    "# Z0_first = pairs[\"base_first\"]\n",
    "# Z0_last = pairs[\"base_last\"]\n",
    "# Z0_final = pairs[\"base_final\"]\n",
    "# Z1_best_first = pairs[\"fine_tuned_first\"]\n",
    "# Z1_best_last = pairs[\"fine_tuned_last\"]\n",
    "# Z1_best_final = pairs[\"fine_tuned_final\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC73JnqmfS6c"
   },
   "source": [
    "##5. Calculating Linear Transformation Matrix and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8d52Cqj-fbFA"
   },
   "outputs": [],
   "source": [
    "Z0_first_np = Z0_first.cpu().numpy()\n",
    "Z0_last_np = Z0_last.cpu().numpy()\n",
    "Z0_final_np = Z0_final.cpu().numpy()\n",
    "\n",
    "Z1_best_first_np = Z1_best_first.cpu().numpy()\n",
    "Z1_best_last_np = Z1_best_last.cpu().numpy()\n",
    "Z1_best_final_np = Z1_best_final.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cbIVuUenfc39"
   },
   "outputs": [],
   "source": [
    "# Add constant 1 column for bias term\n",
    "def constant1_full(Z0):\n",
    "  ones = np.ones((Z0.shape[0], 1)) # (1,1)\n",
    "  return np.hstack([Z0, ones])\n",
    "\n",
    "Z0_first_aug = constant1_full(Z0_first_np)\n",
    "Z0_last_aug = constant1_full(Z0_last_np)\n",
    "Z0_final_aug = constant1_full(Z0_final_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Fd98ngXpfkas"
   },
   "outputs": [],
   "source": [
    "# Calculate Bias b and Linear Transformation Matrix W\n",
    "def leastSquares(Z0, Z1):\n",
    "  W_full, residuals, rank, s = np.linalg.lstsq(Z0, Z1, rcond=None)\n",
    "  return W_full\n",
    "\n",
    "W_full = leastSquares(Z0_first_aug, Z1_best_first_np)  # base_first -> fine-tuned first\n",
    "W_0_1 = W_full[:-1]\n",
    "b_0_1 = W_full[-1]\n",
    "\n",
    "W_full = leastSquares(Z0_first_aug, Z1_best_last_np)   # base_first -> fine-tuned last\n",
    "W_1_1 = W_full[:-1]\n",
    "b_1_1 = W_full[-1]\n",
    "\n",
    "W_full = leastSquares(Z0_first_aug, Z1_best_final_np)  # base_first -> fine-tuned final\n",
    "W_2_1 = W_full[:-1]\n",
    "b_2_1 = W_full[-1]\n",
    "\n",
    "W_full = leastSquares(Z0_last_aug, Z1_best_last_np)    # base_last -> fine-tuned last\n",
    "W_3_1 = W_full[:-1]\n",
    "b_3_1 = W_full[-1]\n",
    "\n",
    "W_full = leastSquares(Z0_last_aug, Z1_best_final_np)   # base_last -> fine-tuned final\n",
    "W_4_1 = W_full[:-1]\n",
    "b_4_1 = W_full[-1]\n",
    "\n",
    "W_full = leastSquares(Z0_final_aug, Z1_best_final_np)  # base_final -> fine-tuned final\n",
    "W_5_1 = W_full[:-1]\n",
    "b_5_1 = W_full[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "x812VR4KewOD",
    "outputId": "ee332298-41da-4f74-d221-131ccb622c7f"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"W_first_first\": W_0_1,\n",
    "    \"b_first_first\": b_0_1,\n",
    "    \"W_first_last\": W_1_1,\n",
    "    \"b_first_last\": b_1_1,\n",
    "    \"W_first_final\": W_2_1,\n",
    "    \"b_first_final\": b_2_1,\n",
    "    \"W_last_last\": W_3_1,\n",
    "    \"b_last_last\": b_3_1,\n",
    "    \"W_last_final\": W_4_1,\n",
    "    \"b_last_final\": b_4_1,\n",
    "    \"W_final_final\": W_5_1,\n",
    "    \"b_final_final\": b_5_1\n",
    "}, \"clip_mnist_transformations.pt\")\n",
    "\n",
    "# Code to Load:\n",
    "# vals = torch.load(\"clip_mnist_transformations.pt\")\n",
    "# W_0_1 = vals[\"W_first_first\"]\n",
    "# b_0_1 = vals[\"b_first_first\"]\n",
    "# W_1_1 = vals[\"W_first_last\"]\n",
    "# b_1_1 = vals[\"b_first_last\"]\n",
    "# W_2_1 = vals[\"W_first_final\"]\n",
    "# b_2_1 = vals[\"b_first_final\"]\n",
    "# W_3_1 = vals[\"W_last_last\"]\n",
    "# b_3_1 = vals[\"b_last_last\"]\n",
    "# W_4_1 = vals[\"W_last_final\"]\n",
    "# b_4_1 = vals[\"b_last_final\"]\n",
    "# W_5_1 = vals[\"W_final_final\"]\n",
    "# b_5_1 = vals[\"b_final_final\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-vRPjMyfmYq"
   },
   "source": [
    "##6. Augmenting Base CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-3tqeLn1qMhR"
   },
   "outputs": [],
   "source": [
    "class TransformedCLIP(nn.Module):\n",
    "  def __init__(self, clip_model, W=None, b=None, transform_stage=\"\", add_classifier=False):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "    self.b = torch.from_numpy(b.astype(np.float32)).to(device) if b is not None else None\n",
    "    self.transform_stage = transform_stage\n",
    "    # base_first -> fine-tuned_first\n",
    "    # base_first -> fine-tuned_last\n",
    "    # base_first -> fine-tuned_final\n",
    "    # base_last -> fine-tuned_last\n",
    "    # base_last -> fine-tuned_final\n",
    "    # base_final -> fine-tuned_final\n",
    "    self.add_classifier = add_classifier\n",
    "    if self.add_classifier:\n",
    "        self.classifier = nn.Linear(512, 10) # Mapping 512 image embeddings to 10 for (0-9 MNIST Classification)\n",
    "\n",
    "  def customPass(self, image):\n",
    "    image = image.to(device) # [B, 3, 224, 224]\n",
    "    x = self.clip.visual.conv1(image) # [B, 768, 7, 7]\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # [B, width, 49]\n",
    "    x = x.permute(0, 2, 1) # [B, 49, 768]\n",
    "\n",
    "    cls_token = self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "    x = torch.cat([cls_token, x], dim=1)  # [B, 50, 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "    x = self.clip.visual.ln_pre(x)  # [B, 50, 768]\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, B, 768] # NLD -> LND\n",
    "    for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "      x = block(x)\n",
    "      if i == 0 and (self.transform_stage in {\"first-first\", \"first-last\", \"first-final\"}): # After first transformer block\n",
    "        if self.transform_stage == \"first-final\":  # Need to project first layer into final embedding space (512-d vector) before manipulation\n",
    "          # x = x.permute(1,0,2) # [B, 50, 768]; LND -> NLD\n",
    "          # cls_token = x[:,0,:]\n",
    "          # cls_token = self.clip.visual.ln_post(cls_token)\n",
    "\n",
    "          # if self.clip.visual.proj is not None:\n",
    "          #   cls_token = cls_token @ self.clip.visual.proj.to(x.dtype)\n",
    "          # else:\n",
    "          #   raise ValueError(\"Projection layer is None. Expected [768, 512]\")\n",
    "\n",
    "          # cls_token = cls_token @ self.W + self.b\n",
    "\n",
    "          # return cls_token\n",
    "          cls = x[0, :, :] # [B, 768]; Raw CLS after first block (768 dimensions)\n",
    "          cls = cls.to(torch.float32)\n",
    "          out = cls @ self.W + self.b # [B, 512] # Apply learned 768-> 512 W and bias\n",
    "          return out\n",
    "\n",
    "        # For first-first\n",
    "        cls_token = x[0,:,:] # [B, 768]\n",
    "        cls_token = cls_token.to(torch.float32)\n",
    "        cls_token = cls_token @ self.W + self.b\n",
    "        x = torch.cat([cls_token.unsqueeze(0), x[1:]], dim=0) # Replace CLS Token\n",
    "\n",
    "        if self.transform_stage == \"first-last\":\n",
    "          break\n",
    "\n",
    "      if i == len(self.clip.visual.transformer.resblocks) - 1 and (self.transform_stage == \"last-last\" or self.transform_stage == \"last-final\"):\n",
    "        if self.transform_stage == \"last-final\": # Need to project last layer into final embedding space (512-d vector) before manipulation\n",
    "          #  x = x.permute(1,0,2) # [B, 50, 768]; LND -> NLD\n",
    "          #  cls_token = x[:,0,:]\n",
    "          #  cls_token = self.clip.visual.ln_post(cls_token) \n",
    "\n",
    "          #  if self.clip.visual.proj is not None:\n",
    "          #    cls_token = cls_token @ self.clip.visual.proj.to(x.dtype)\n",
    "          #  else:\n",
    "          #    raise ValueError(\"Projection layer is None. Expected [768, 512]\")\n",
    "\n",
    "          #  cls_token = cls_token @ self.W + self.b\n",
    "\n",
    "          #  return cls_token\n",
    "          cls = x[0, :, :] # [B, 768]; Raw CLS from last block (768 dimensions)\n",
    "          cls = cls.to(torch.float32)\n",
    "          out = cls @ self.W + self.b # [B, 512]\n",
    "          return out\n",
    "\n",
    "        # For last-last\n",
    "        cls_token = x[0,:,:] # [B, 768]\n",
    "        cls_token = cls_token.to(torch.float32)\n",
    "        cls_token = cls_token @ self.W + self.b\n",
    "        x = torch.cat([cls_token.unsqueeze(0), x[1:]], dim=0) # Replaces CLS Token and adds it to shape of this [49, B, 768] <- all tokens after CLS (excluding CLS) -> [50, B, 768]\n",
    "\n",
    "    x = x.permute(1,0,2) # [B, 50, 768]; LND -> NLD\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None:\n",
    "      x = x @ self.clip.visual.proj\n",
    "\n",
    "    return x\n",
    "\n",
    "  def forward(self, image):\n",
    "    image = image.to(device)\n",
    "    if self.transform_stage == \"final-final\":\n",
    "      image_embed = self.clip.encode_image(image)\n",
    "      image_embed = image_embed @ self.W.T + self.b\n",
    "    elif self.transform_stage in {\"first-first\", \"first-last\", \"first-final\", \"last-last\", \"last-final\"}:\n",
    "      image_embed = self.customPass(image)\n",
    "    else: # From CLIP GitHub -> model.py @ 359. https://github.com/openai/CLIP/blob/main/clip/model.py\n",
    "      image_embed = self.clip.encode_image(image)\n",
    "\n",
    "    # Don't need to do normalization from original code. Need to preserve scale information to accurately fit W, b. Keep the true structural nature\n",
    "    # Can normalize if evaluating cosine similarities between embeddings. Normalize both base and fine-tuned if doign so before computing similarity/dot product.\n",
    "    # Just doing classification so no need for text\n",
    "\n",
    "    if self.add_classifier:\n",
    "        logits = self.classifier(image_embed)\n",
    "        return logits\n",
    "    else:\n",
    "        return image_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "dfqBIe2nppF1",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (clip): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIP_aug_first_first, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_first_first = CLIP_aug_first_first.float()\n",
    "CLIP_aug_first_first = TransformedCLIP(CLIP_aug_first_first, W_0_1, b_0_1, transform_stage=\"first-first\", add_classifier=True).to(device)\n",
    "CLIP_aug_first_first.eval()\n",
    "\n",
    "CLIP_aug_first_last, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_first_last = CLIP_aug_first_last.float()\n",
    "CLIP_aug_first_last = TransformedCLIP(CLIP_aug_first_last, W_1_1, b_1_1, transform_stage=\"first-last\", add_classifier=True).to(device)\n",
    "CLIP_aug_first_last.eval()\n",
    "\n",
    "CLIP_aug_first_final, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_first_final = CLIP_aug_first_final.float()\n",
    "CLIP_aug_first_final = TransformedCLIP(CLIP_aug_first_final, W_2_1, b_2_1, transform_stage=\"first-final\", add_classifier=True).to(device)\n",
    "CLIP_aug_first_final.eval()\n",
    "\n",
    "CLIP_aug_last_last, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_last_last = CLIP_aug_last_last.float()\n",
    "CLIP_aug_last_last.eval()\n",
    "CLIP_aug_last_last = TransformedCLIP(CLIP_aug_last_last, W_3_1, b_3_1, transform_stage=\"last-last\", add_classifier=True).to(device)\n",
    "\n",
    "CLIP_aug_last_final, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_last_final = CLIP_aug_last_final.float()\n",
    "CLIP_aug_last_final = TransformedCLIP(CLIP_aug_last_final, W_4_1, b_4_1, transform_stage=\"last-final\", add_classifier=True).to(device)\n",
    "CLIP_aug_last_final.eval()\n",
    "\n",
    "CLIP_aug_final_final, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_final_final = CLIP_aug_final_final.float()\n",
    "CLIP_aug_final_final = TransformedCLIP(CLIP_aug_final_final, W_5_1, b_5_1, transform_stage=\"final-final\", add_classifier=True).to(device)\n",
    "CLIP_aug_final_final.eval()\n",
    "\n",
    "CLIP_base, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_base = CLIP_base.float()\n",
    "CLIP_base = TransformedCLIP(CLIP_base, add_classifier=True).to(device)\n",
    "CLIP_base.eval()\n",
    "\n",
    "CLIP_fine_tuned, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_fine_tuned = CLIP_fine_tuned.float()\n",
    "CLIP_fine_tuned = CLIPClassifier(clip_model=CLIP_fine_tuned).to(device)\n",
    "CLIP_fine_tuned.load_state_dict(torch.load(\"best_clip_mnist.pt\", map_location=device))\n",
    "CLIP_fine_tuned.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvqt0_xOs3cO"
   },
   "source": [
    "##7. Evaluating Base v Fine-Tuned vs Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "luiKbXWntZ7h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 157/157 [03:56<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmented First-First Accuracy: 0.1010\n",
      "Augmented First-Last Accuracy: 0.1135\n",
      "Augmented First-Final Accuracy: 0.1009\n",
      "Augmented Last-Last Accuracy: 0.0958\n",
      "Augmented Last-Final Accuracy: 0.1009\n",
      "Augmented Final-Final Accuracy: 0.0982\n",
      "Base Accuracy: 0.0909\n",
      "Fine-Tuned Accuracy: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_first_first = 0\n",
    "correct_first_last = 0\n",
    "correct_first_final = 0\n",
    "correct_last_last = 0\n",
    "correct_last_final = 0\n",
    "correct_final_final = 0\n",
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    images = batch[\"pixel_values\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    # First-First Augmented Model\n",
    "    logits_first_first = CLIP_aug_first_first(images)\n",
    "    pred_first_first = logits_first_first.argmax(dim=1)\n",
    "    correct_first_first += (pred_first_first == labels).sum().item()\n",
    "\n",
    "    # First-Last Augmented Model\n",
    "    logits_first_last = CLIP_aug_first_last(images)\n",
    "    pred_first_last = logits_first_last.argmax(dim=1)\n",
    "    correct_first_last += (pred_first_last == labels).sum().item()\n",
    "\n",
    "    # First-Final Augmented Model\n",
    "    logits_first_final = CLIP_aug_first_final(images)\n",
    "    pred_first_final = logits_first_final.argmax(dim=1)\n",
    "    correct_first_final += (pred_first_final == labels).sum().item()\n",
    "\n",
    "    # Last-Last Augmented Model\n",
    "    logits_last_last = CLIP_aug_last_last(images)\n",
    "    pred_last_last = logits_last_last.argmax(dim=1)\n",
    "    correct_last_last += (pred_last_last == labels).sum().item()\n",
    "\n",
    "    # Last-Final Augmented Model\n",
    "    logits_last_final = CLIP_aug_last_final(images)\n",
    "    pred_last_final = logits_last_final.argmax(dim=1)\n",
    "    correct_last_final += (pred_last_final == labels).sum().item()\n",
    "\n",
    "    # Final-Final Augmented Model\n",
    "    logits_final_final = CLIP_aug_final_final(images)\n",
    "    pred_final_final = logits_final_final.argmax(dim=1)\n",
    "    correct_final_final += (pred_final_final == labels).sum().item()\n",
    "\n",
    "    # Base Model\n",
    "    logits_base = CLIP_base(images)\n",
    "    pred_base = logits_base.argmax(dim=1)\n",
    "    correct_base += (pred_base == labels).sum().item()\n",
    "\n",
    "    # Fine-Tuned Model\n",
    "    logits_fine_tuned = CLIP_fine_tuned(images)\n",
    "    pred_fine_tuned = logits_fine_tuned.argmax(dim=1)\n",
    "    correct_fine_tuned += (pred_fine_tuned == labels).sum().item()\n",
    "\n",
    "first_first_acc = correct_first_first / total_samples\n",
    "first_last_acc = correct_first_last / total_samples\n",
    "first_final_acc = correct_first_final / total_samples\n",
    "last_last_acc = correct_last_last / total_samples\n",
    "last_final_acc = correct_last_final / total_samples\n",
    "final_final_acc = correct_final_final / total_samples\n",
    "base_acc = correct_base / total_samples\n",
    "fine_tuned_acc = correct_fine_tuned / total_samples\n",
    "\n",
    "print(f\"\\nAugmented First-First Accuracy: {first_first_acc:.4f}\")\n",
    "print(f\"Augmented First-Last Accuracy: {first_last_acc:.4f}\")\n",
    "print(f\"Augmented First-Final Accuracy: {first_final_acc:.4f}\")\n",
    "print(f\"Augmented Last-Last Accuracy: {last_last_acc:.4f}\")\n",
    "print(f\"Augmented Last-Final Accuracy: {last_final_acc:.4f}\")\n",
    "print(f\"Augmented Final-Final Accuracy: {final_final_acc:.4f}\")\n",
    "print(f\"Base Accuracy: {base_acc:.4f}\")\n",
    "print(f\"Fine-Tuned Accuracy: {fine_tuned_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuned classifier head\n",
    "class TransformedCLIP1(nn.Module):\n",
    "  def __init__(self, clip_model, W=None, b=None, transform_stage=\"\", classifier_head=None):\n",
    "    super().__init__()\n",
    "    self.clip = clip_model\n",
    "    self.W = torch.from_numpy(W.astype(np.float32)).to(device) if W is not None else None\n",
    "    self.b = torch.from_numpy(b.astype(np.float32)).to(device) if b is not None else None\n",
    "    self.transform_stage = transform_stage\n",
    "    # base_first -> fine-tuned_first\n",
    "    # base_first -> fine-tuned_last\n",
    "    # base_first -> fine-tuned_final\n",
    "    # base_last -> fine-tuned_last\n",
    "    # base_last -> fine-tuned_final\n",
    "    # base_final -> fine-tuned_final\n",
    "    if classifier_head is not None:\n",
    "        self.classifier = classifier_head\n",
    "    else:\n",
    "        self.classifier = None\n",
    "\n",
    "  def customPass(self, image):\n",
    "    image = image.to(device) # [B, 3, 224, 224]\n",
    "    x = self.clip.visual.conv1(image) # [B, 768, 7, 7]\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1) # [B, width, 49]\n",
    "    x = x.permute(0, 2, 1) # [B, 49, 768]\n",
    "\n",
    "    cls_token = self.clip.visual.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "    x = torch.cat([cls_token, x], dim=1)  # [B, 50, 768]\n",
    "    x = x + self.clip.visual.positional_embedding.to(x.dtype)\n",
    "    x = self.clip.visual.ln_pre(x)  # [B, 50, 768]\n",
    "\n",
    "    x = x.permute(1,0,2) # [50, B, 768] # NLD -> LND\n",
    "    for i, block in enumerate(self.clip.visual.transformer.resblocks):\n",
    "      x = block(x)\n",
    "      if i == 0 and (self.transform_stage in {\"first-first\", \"first-last\", \"first-final\"}): # After first transformer block\n",
    "        if self.transform_stage == \"first-final\":  # Need to project first layer into final embedding space (512-d vector) before manipulation\n",
    "          # x = x.permute(1,0,2) # [B, 50, 768]; LND -> NLD\n",
    "          # cls_token = x[:,0,:]\n",
    "          # cls_token = self.clip.visual.ln_post(cls_token)\n",
    "\n",
    "          # if self.clip.visual.proj is not None:\n",
    "          #   cls_token = cls_token @ self.clip.visual.proj.to(x.dtype)\n",
    "          # else:\n",
    "          #   raise ValueError(\"Projection layer is None. Expected [768, 512]\")\n",
    "\n",
    "          # cls_token = cls_token @ self.W + self.b\n",
    "\n",
    "          # return cls_token\n",
    "          cls = x[0, :, :] # [B, 768]; Raw CLS after first block (768 dimensions)\n",
    "          cls = cls.to(torch.float32)\n",
    "          out = cls @ self.W + self.b # [B, 512] # Apply learned 768-> 512 W and bias\n",
    "          return out\n",
    "\n",
    "        # For first-first\n",
    "        cls_token = x[0,:,:] # [B, 768]\n",
    "        cls_token = cls_token.to(torch.float32)\n",
    "        cls_token = cls_token @ self.W + self.b\n",
    "        x = torch.cat([cls_token.unsqueeze(0), x[1:]], dim=0) # Replace CLS Token\n",
    "\n",
    "        if self.transform_stage == \"first-last\":\n",
    "          break\n",
    "\n",
    "      if i == len(self.clip.visual.transformer.resblocks) - 1 and (self.transform_stage == \"last-last\" or self.transform_stage == \"last-final\"):\n",
    "        if self.transform_stage == \"last-final\": # Need to project last layer into final embedding space (512-d vector) before manipulation\n",
    "          #  x = x.permute(1,0,2) # [B, 50, 768]; LND -> NLD\n",
    "          #  cls_token = x[:,0,:]\n",
    "          #  cls_token = self.clip.visual.ln_post(cls_token) \n",
    "\n",
    "          #  if self.clip.visual.proj is not None:\n",
    "          #    cls_token = cls_token @ self.clip.visual.proj.to(x.dtype)\n",
    "          #  else:\n",
    "          #    raise ValueError(\"Projection layer is None. Expected [768, 512]\")\n",
    "\n",
    "          #  cls_token = cls_token @ self.W + self.b\n",
    "\n",
    "          #  return cls_token\n",
    "          cls = x[0, :, :] # [B, 768]; Raw CLS from last block (768 dimensions)\n",
    "          cls = cls.to(torch.float32)\n",
    "          out = cls @ self.W + self.b # [B, 512]\n",
    "          return out\n",
    "\n",
    "        # For last-last\n",
    "        cls_token = x[0,:,:] # [B, 768]\n",
    "        cls_token = cls_token.to(torch.float32)\n",
    "        cls_token = cls_token @ self.W + self.b\n",
    "        x = torch.cat([cls_token.unsqueeze(0), x[1:]], dim=0) # Replaces CLS Token and adds it to shape of this [49, B, 768] <- all tokens after CLS (excluding CLS) -> [50, B, 768]\n",
    "\n",
    "    x = x.permute(1,0,2) # [B, 50, 768]; LND -> NLD\n",
    "    x = self.clip.visual.ln_post(x[:, 0, :])\n",
    "\n",
    "    if self.clip.visual.proj is not None:\n",
    "      x = x @ self.clip.visual.proj\n",
    "\n",
    "    return x\n",
    "\n",
    "  def forward(self, image):\n",
    "    image = image.to(device)\n",
    "    if self.transform_stage == \"final-final\":\n",
    "      image_embed = self.clip.encode_image(image)\n",
    "      image_embed = image_embed @ self.W + self.b\n",
    "    elif self.transform_stage in {\"first-first\", \"first-last\", \"first-final\", \"last-last\", \"last-final\"}:\n",
    "      image_embed = self.customPass(image)\n",
    "    else: # From CLIP GitHub -> model.py @ 359. https://github.com/openai/CLIP/blob/main/clip/model.py\n",
    "      image_embed = self.clip.encode_image(image)\n",
    "\n",
    "    # Don't need to do normalization from original code. Need to preserve scale information to accurately fit W, b. Keep the true structural nature\n",
    "    # Can normalize if evaluating cosine similarities between embeddings. Normalize both base and fine-tuned if doign so before computing similarity/dot product.\n",
    "    # Just doing classification so no need for text\n",
    "\n",
    "    if self.classifier is not None:\n",
    "        return self.classifier(image_embed)\n",
    "    else:\n",
    "        return image_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformedCLIP1(\n",
       "  (clip): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIP_base, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_base = CLIP_base.float()\n",
    "CLIP_base = TransformedCLIP1(CLIP_base, classifier_head=nn.Linear(512, 10)).to(device)\n",
    "CLIP_base.eval()\n",
    "\n",
    "CLIP_fine_tuned, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_fine_tuned = CLIP_fine_tuned.float()\n",
    "CLIP_fine_tuned = CLIPClassifier(clip_model=CLIP_fine_tuned).to(device)\n",
    "CLIP_fine_tuned.load_state_dict(torch.load(\"best_clip_mnist.pt\", map_location=device))\n",
    "CLIP_fine_tuned.eval()\n",
    "\n",
    "CLIP_aug_first_first, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_first_first = CLIP_aug_first_first.float()\n",
    "CLIP_aug_first_first = TransformedCLIP1(CLIP_aug_first_first, W_0_1, b_0_1, transform_stage=\"first-first\", classifier_head = CLIP_fine_tuned.classifier).to(device)\n",
    "CLIP_aug_first_first.eval()\n",
    "\n",
    "CLIP_aug_first_last, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_first_last = CLIP_aug_first_last.float()\n",
    "CLIP_aug_first_last = TransformedCLIP1(CLIP_aug_first_last, W_1_1, b_1_1, transform_stage=\"first-last\", classifier_head = CLIP_fine_tuned.classifier).to(device)\n",
    "CLIP_aug_first_last.eval()\n",
    "\n",
    "CLIP_aug_first_final, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_first_final = CLIP_aug_first_final.float()\n",
    "CLIP_aug_first_final = TransformedCLIP1(CLIP_aug_first_final, W_2_1, b_2_1, transform_stage=\"first-final\", classifier_head = CLIP_fine_tuned.classifier).to(device)\n",
    "CLIP_aug_first_final.eval()\n",
    "\n",
    "CLIP_aug_last_last, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_last_last = CLIP_aug_last_last.float()\n",
    "CLIP_aug_last_last.eval()\n",
    "CLIP_aug_last_last = TransformedCLIP1(CLIP_aug_last_last, W_3_1, b_3_1, transform_stage=\"last-last\", classifier_head = CLIP_fine_tuned.classifier).to(device)\n",
    "\n",
    "CLIP_aug_last_final, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_last_final = CLIP_aug_last_final.float()\n",
    "CLIP_aug_last_final = TransformedCLIP1(CLIP_aug_last_final, W_4_1, b_4_1, transform_stage=\"last-final\", classifier_head = CLIP_fine_tuned.classifier).to(device)\n",
    "CLIP_aug_last_final.eval()\n",
    "\n",
    "CLIP_aug_final_final, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "CLIP_aug_final_final = CLIP_aug_final_final.float()\n",
    "CLIP_aug_final_final = TransformedCLIP1(CLIP_aug_final_final, W_5_1, b_5_1, transform_stage=\"final-final\", classifier_head = CLIP_fine_tuned.classifier).to(device)\n",
    "CLIP_aug_final_final.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 157/157 [03:55<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmented First-First Accuracy: 0.1171\n",
      "Augmented First-Last Accuracy: 0.1135\n",
      "Augmented First-Final Accuracy: 0.1135\n",
      "Augmented Last-Last Accuracy: 0.1135\n",
      "Augmented Last-Final Accuracy: 0.1135\n",
      "Augmented Final-Final Accuracy: 0.1135\n",
      "Base Accuracy: 0.0806\n",
      "Fine-Tuned Accuracy: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_first_first = 0\n",
    "correct_first_last = 0\n",
    "correct_first_final = 0\n",
    "correct_last_last = 0\n",
    "correct_last_final = 0\n",
    "correct_final_final = 0\n",
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "    images = batch[\"pixel_values\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "    # First-First Augmented Model\n",
    "    logits_first_first = CLIP_aug_first_first(images)\n",
    "    pred_first_first = logits_first_first.argmax(dim=1)\n",
    "    correct_first_first += (pred_first_first == labels).sum().item()\n",
    "\n",
    "    # First-Last Augmented Model\n",
    "    logits_first_last = CLIP_aug_first_last(images)\n",
    "    pred_first_last = logits_first_last.argmax(dim=1)\n",
    "    correct_first_last += (pred_first_last == labels).sum().item()\n",
    "\n",
    "    # First-Final Augmented Model\n",
    "    logits_first_final = CLIP_aug_first_final(images)\n",
    "    pred_first_final = logits_first_final.argmax(dim=1)\n",
    "    correct_first_final += (pred_first_final == labels).sum().item()\n",
    "\n",
    "    # Last-Last Augmented Model\n",
    "    logits_last_last = CLIP_aug_last_last(images)\n",
    "    pred_last_last = logits_last_last.argmax(dim=1)\n",
    "    correct_last_last += (pred_last_last == labels).sum().item()\n",
    "\n",
    "    # Last-Final Augmented Model\n",
    "    logits_last_final = CLIP_aug_last_final(images)\n",
    "    pred_last_final = logits_last_final.argmax(dim=1)\n",
    "    correct_last_final += (pred_last_final == labels).sum().item()\n",
    "\n",
    "    # Final-Final Augmented Model\n",
    "    logits_final_final = CLIP_aug_final_final(images)\n",
    "    pred_final_final = logits_final_final.argmax(dim=1)\n",
    "    correct_final_final += (pred_final_final == labels).sum().item()\n",
    "\n",
    "    # Base Model\n",
    "    logits_base = CLIP_base(images)\n",
    "    pred_base = logits_base.argmax(dim=1)\n",
    "    correct_base += (pred_base == labels).sum().item()\n",
    "\n",
    "    # Fine-Tuned Model\n",
    "    logits_fine_tuned = CLIP_fine_tuned(images)\n",
    "    pred_fine_tuned = logits_fine_tuned.argmax(dim=1)\n",
    "    correct_fine_tuned += (pred_fine_tuned == labels).sum().item()\n",
    "\n",
    "first_first_acc = correct_first_first / total_samples\n",
    "first_last_acc = correct_first_last / total_samples\n",
    "first_final_acc = correct_first_final / total_samples\n",
    "last_last_acc = correct_last_last / total_samples\n",
    "last_final_acc = correct_last_final / total_samples\n",
    "final_final_acc = correct_final_final / total_samples\n",
    "base_acc = correct_base / total_samples\n",
    "fine_tuned_acc = correct_fine_tuned / total_samples\n",
    "\n",
    "print(f\"\\nAugmented First-First Accuracy: {first_first_acc:.4f}\")\n",
    "print(f\"Augmented First-Last Accuracy: {first_last_acc:.4f}\")\n",
    "print(f\"Augmented First-Final Accuracy: {first_final_acc:.4f}\")\n",
    "print(f\"Augmented Last-Last Accuracy: {last_last_acc:.4f}\")\n",
    "print(f\"Augmented Last-Final Accuracy: {last_final_acc:.4f}\")\n",
    "print(f\"Augmented Final-Final Accuracy: {final_final_acc:.4f}\")\n",
    "print(f\"Base Accuracy: {base_acc:.4f}\")\n",
    "print(f\"Fine-Tuned Accuracy: {fine_tuned_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_dataset(\"ylecun/mnist\")\n",
    "train = mnist[\"train\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Hqn56BjwxmpC",
    "SC73JnqmfS6c",
    "f-vRPjMyfmYq",
    "Uvqt0_xOs3cO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
