{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Overview\n",
    "\n",
    "Author: Darrin O'Brien, email: darrinobrien5@gmail.com\n",
    "\n",
    "1. Fine-Tunes CLIP ViT-32 on the subset of images and labels equal to 0 from the MNIST dataset. \n",
    "2. Evaluates the performance of the fine-tuned model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-Tuning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Installs for Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install -U transformers datasets\n",
    "!pip install fifty regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install matplotlib\n",
    "!pip install -U pillow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir scipy datasets # Only needed within runpod environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 # only needed for runpod environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import clip \n",
    "import numpy as np \n",
    "from datasets import load_dataset \n",
    "from tqdm import tqdm \n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model = clip_model.float()\n",
    "mnist = load_dataset(\"ylecun/mnist\")\n",
    "split = mnist[\"train\"].train_test_split(test_size=0.2, seed=66)\n",
    "\n",
    "train_dataset = split[\"train\"].filter(lambda example: example[\"label\"] == 0)\n",
    "val_dataset = split[\"test\"].filter(lambda example: example[\"label\"] == 0)\n",
    "test_dataset = mnist[\"test\"].filter(lambda example: example[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "val_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])\n",
    "test_dataset.set_format(type=\"python\", columns=[\"image\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_fn(batch):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "\n",
    "    for item in batch:\n",
    "        img = preprocess(item[\"image\"])\n",
    "        images.append(img)\n",
    "        labels.append(item[\"label\"])\n",
    "    \n",
    "    images = torch.stack(images) # [Batch, channels, height, width] -> [64, 3, 224, 224]\n",
    "    labels = torch.tensor(labels, dtype=torch.long) # 64 bit integer, pytorch tensor\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": images.to(device),\n",
    "        \"labels\": labels.to(device)\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=clip_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=clip_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapping CLIP for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier0(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = nn.Linear(self.clip.visual.output_dim, num_classes) # 512 -> 2\n",
    "    \n",
    "    def forward(self, images):\n",
    "        image_features = self.clip.encode_image(images)\n",
    "        logits = self.classifier(image_features)\n",
    "        return logits \n",
    "\n",
    "model = CLIPClassifier0(clip_model=clip_model).to(device)\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 15\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tuning CLIP on MNIST 0 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_steps = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "    \n",
    "    avg_train_loss = total_train_loss / train_steps\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_steps = 0\n",
    "    total_val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch[\"pixel_values\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "            pred = logits.argmax(dim=1)\n",
    "\n",
    "            total_val_correct += (pred == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "    \n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    val_classification_acc = total_val_correct / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f} | Validation Accuracy {val_classification_acc:.4f}\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f} (Epoch {best_epoch})\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), \"best_clip_mnist_0_fp32.pt\")\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating Fine-Tuned Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "base = base.float()\n",
    "model = CLIPClassifier0(clip_model=base).to(device)\n",
    "\n",
    "fine_tuned, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "fine_tuned = fine_tuned.float()\n",
    "fine_tuned_model = CLIPClassifier0(clip_model=fine_tuned).to(device)\n",
    "fine_tuned_model.load_state_dict(torch.load(\"best_clip_mnist_0_fp32\"))\n",
    "\n",
    "model.eval()\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "loss_base = 0\n",
    "total_base = 0\n",
    "loss_fine_tuned = 0\n",
    "total_fine_tuned = 0\n",
    "\n",
    "correct_base = 0\n",
    "correct_fine_tuned = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Base Model\n",
    "        logits_base = model(images)\n",
    "        loss_base = criterion(logits_base, labels).item()\n",
    "        total_base += 1\n",
    "        correct_base += (logits_base.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        # Fine-Tuned Model\n",
    "        logits_fine_tuned = fine_tuned_model(images)\n",
    "        loss_fine_tuned = criterion(logits_fine_tuned, labels).item()\n",
    "        total_fine_tuned += 1\n",
    "        correct_fine_tuned += (logits_fine_tuned.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "avg_base_loss = loss_base / total_base\n",
    "avg_fine_tuned_loss = loss_fine_tuned / total_fine_tuned \n",
    "\n",
    "base_acc = correct_base / total_samples \n",
    "fine_tuned_acc = correct_fine_tuned / total_samples \n",
    "\n",
    "print(f\"\\nAverage Base Loss: {avg_base_loss:.4f}, Base Classification Accuracy: {base_acc:.4f}\")\n",
    "print(f\"Average Fine-Tuned Loss: {avg_fine_tuned_loss:.4f}, Fine-Tuned Classification: {fine_tuned_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
